# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c9077GMd1GNE3oTfPrCtb8nFjy9jRj0O
"""

pip install gradio

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df_train = pd.read_csv('fraudTrain.csv', index_col=0)
df_test = pd.read_csv('fraudTest.csv',index_col=0)

df_train.head()

df_train.columns

row,col=df_train.shape
print(f'The dataset has {row} rows and {col} columns.')

print(df_train.describe())

df_train.info()

fraud_percentage=len(df_train[df_train['is_fraud']==1])/len(df_train)*100
non_fraud_percentage=100-fraud_percentage
print(f'The percentage of fraud is {fraud_percentage} % and the percentage of non-fraud is {non_fraud_percentage} %')
print('This shows how imbalanced is the dataset')

df_train.isna().sum()

len(df_train)==len(df_train.drop_duplicates())

df_train.head()

df_train['cc_num'].value_counts()

df_train[df_train['is_fraud']==1].cc_num.value_counts()

df_train.drop(columns=['cc_num'],inplace=True)
df_test.drop(columns=['cc_num'],inplace=True)

df_train.merchant.value_counts()

df_train[df_train['is_fraud']==1].merchant.value_counts().head(50)

df_train.drop(columns=['merchant'],inplace=True)
df_test.drop(columns=['merchant'],inplace=True)

df_train.category.value_counts()

df_train[df_train['is_fraud']==1].category.value_counts()

(1743+1713+915+843+618)/(df_train[df_train['is_fraud']==1].category.value_counts().sum())*100

df_train.groupby('category')['amt'].sum().sort_values(ascending=False)

(14460822.38+9307993.61+8625149.68+8351732.29+7173928.11)/(df_train.groupby('category')['amt'].sum().sum())*100
!pip install gradio

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df_train = pd.read_csv('fraudTrain.csv', index_col=0)
df_test = pd.read_csv('fraudTest.csv',index_col=0)

df_train.head()

df_train.columns

row,col=df_train.shape
print(f'The dataset has {row} rows and {col} columns.')

print(df_train.describe())

df_train.info()

fraud_percentage=len(df_train[df_train['is_fraud']==1])/len(df_train)*100
non_fraud_percentage=100-fraud_percentage
print(f'The percentage of fraud is {fraud_percentage} % and the percentage of non-fraud is {non_fraud_percentage} %')
print('This shows how imbalanced is the dataset')

df_train.isna().sum()

len(df_train)==len(df_train.drop_duplicates())

df_train.head()

df_train['cc_num'].value_counts()

df_train[df_train['is_fraud']==1].cc_num.value_counts()

df_train.drop(columns=['cc_num'],inplace=True)
df_test.drop(columns=['cc_num'],inplace=True)

df_train.merchant.value_counts()

df_train[df_train['is_fraud']==1].merchant.value_counts().head(50)

df_train.drop(columns=['merchant'],inplace=True)
df_test.drop(columns=['merchant'],inplace=True)

df_train.category.value_counts()

df_train[df_train['is_fraud']==1].category.value_counts()

(1743+1713+915+843+618)/(df_train[df_train['is_fraud']==1].category.value_counts().sum())*100

df_train.groupby('category')['amt'].sum().sort_values(ascending=False)

(14460822.38+9307993.61+8625149.68+8351732.29+7173928.11)/(df_train.groupby('category')['amt'].sum().sum())*100

def convert_to_other(instance):
    if(instance in ['grocery_pos', 'shopping_net', 'misc_net','shopping_pos', 'gas_transport']):
        return instance
    else :
        return 'others'
df_train['category']=df_train['category'].apply(convert_to_other)
df_test['category']=df_test['category'].apply(convert_to_other)
df_train.category.value_counts()

df_train.head()

from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(drop='first', sparse_output=False)
# drop='first' removes one column to avoid redundancy which also remove multicollinearity
# if we don't remove the input feature will have depencies. sum of values in the col will be 1 which can effect the performance

encoded_array = encoder.fit_transform(df_train[['category']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category']))

df_train = pd.concat([df_train.drop(columns=['category']), encoded_df], axis=1)

encoded_array = encoder.transform(df_test[['category']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category']))

df_test = pd.concat([df_test.drop(columns=['category']), encoded_df], axis=1)

df_train.drop(columns=['first','last'],inplace=True)
df_test.drop(columns=['first','last'],inplace=True)

def M_1_F_0(instance):
    if(instance=='M'):
        return 1;
    return 0;

df_train['gender']=df_train['gender'].apply(M_1_F_0)
df_test['gender']=df_test['gender'].apply(M_1_F_0)

df_train.street.value_counts()

df_train[df_train.is_fraud==1].city.value_counts()

df_train['zip'].value_counts()

df_train.drop(columns=['street','city','state','zip','dob','trans_num','city_pop'],inplace=True)
df_test.drop(columns=['street','city','state','zip','dob','trans_num','city_pop'],inplace=True)
df_train.head()

df_train.drop(columns=['unix_time'],inplace=True)
df_test.drop(columns=['unix_time'],inplace=True)

df_train.head()

df_train.job.value_counts()

df_train[df_train.is_fraud==1].job.value_counts()

(62+56+53+51+50)/(len(df_train[df_train.is_fraud==1]))*100

df_train.drop(columns=['job'],inplace=True)
df_test.drop(columns=['job'],inplace=True)
df_train.head()

ax=sns.histplot(x='amt',data=df_train[df_train.amt<=1000],hue='is_fraud',stat='percent',multiple='dodge',common_norm=False,bins=25)
ax.set_ylabel('Percentage in Each Type')
ax.set_xlabel('Transaction Amount in USD')
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

df_train[(df_train.amt>=200)& (df_train.is_fraud==1)].amt.sum()/df_train[df_train.is_fraud==1].amt.sum()*100

def is_greater_than_200(amt):
    if(amt>=200):
        return 1;
    return 0;
df_train['is_amt_greater_than_200']=df_train.amt.apply(is_greater_than_200)
df_test['is_amt_greater_than_200']=df_test.amt.apply(is_greater_than_200)

split_sum = df_train[df_train.is_fraud==1].groupby('is_amt_greater_than_200')['amt'].sum()

plt.bar(split_sum.index.astype(str), split_sum.values, color=['blue', 'orange'])

df_train['hour']=pd.to_datetime(df_train['trans_date_trans_time']).dt.hour
df_test['hour']=pd.to_datetime(df_test['trans_date_trans_time']).dt.hour
ax=sns.histplot(data=df_train, x="hour", hue="is_fraud", common_norm=False,stat='percent',multiple='dodge')
ax.set_ylabel('Percentage')
ax.set_xlabel('Time (Hour) in a Day')
plt.xticks(np.arange(0,24,1))
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

df_train[(df_train.is_fraud == 1) & ((df_train.hour <= 3) | (df_train.hour >= 22))]['amt'].sum()/(df_train[df_train.is_fraud==1].amt.sum())*100

def time_category(hour):
    if hour>=22 or hour<=3 :
        return "night";
    else :
        return "day"

df_train['time_category']= df_train['hour'].apply(time_category)
df_test['time_category']= df_test['hour'].apply(time_category)

df_train.time_category.value_counts()

df_train.info()

encoder = OneHotEncoder(drop='first', sparse_output=False)
encoded_array = encoder.fit_transform(df_train[['time_category']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['time_category']))

df_train = pd.concat([df_train.drop(columns=['time_category']), encoded_df], axis=1)

encoded_array = encoder.transform(df_test[['time_category']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['time_category']))

df_test = pd.concat([df_test.drop(columns=['time_category']), encoded_df], axis=1)

df_train.info()

df_train.head()

df_train['day']=pd.to_datetime(df_train['trans_date_trans_time']).dt.dayofweek
df_test['day']=pd.to_datetime(df_test['trans_date_trans_time']).dt.dayofweek
ax=sns.histplot(data=df_train, x="day", hue="is_fraud", common_norm=False,stat='percent',multiple='dodge')
#ax.set_xticklabels(['',"Mon","Tue","Wed","Thu","Fri","Sat","Sun"])
plt.xticks(np.arange(7), ["Mon","Tue","Wed","Thu","Fri","Sat","Sun"]) #set the xtick labels
ax.set_ylabel('Percentage')
ax.set_xlabel('Day of Week')
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

df_train[(df_train.is_fraud == 1) & ((df_train.day >= 5)|(df_train.day==0))]['amt'].sum() / df_train[df_train.is_fraud == 1]['amt'].sum()*100

def category_day(day):
    if(day==0 or day>=5):
        return "cat1";
    else:
        return "cat2"
df_train['category_day']=df_train['day'].apply(category_day)
df_test['category_day']=df_test['day'].apply(category_day)

encoded_array = encoder.fit_transform(df_train[['category_day']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category_day']))

df_train = pd.concat([df_train.drop(columns=['category_day']), encoded_df], axis=1)

encoded_array = encoder.transform(df_test[['category_day']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category_day']))

df_test = pd.concat([df_test.drop(columns=['category_day']), encoded_df], axis=1)

df_test['category_day']=df_test['category_day_cat2']
df_test.drop(columns=['category_day_cat2'],inplace=True)

df_train['category_day']=df_train['category_day_cat2']
df_train.drop(columns=['category_day_cat2'],inplace=True)

df_train.category_day.value_counts()

df_fraud = df_train[df_train['is_fraud'] == 1]  # Keep only fraud transactions

ax = sns.histplot(data=df_fraud, x="day", common_norm=False, stat='percent', multiple='dodge')

ax.set_xticklabels(["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"])
ax.set_ylabel('Percentage')
ax.set_xlabel('Day of Week')
plt.title('Fraud Transactions by Day of the Week')
plt.show()

df_train['month']=pd.to_datetime(df_train['trans_date_trans_time']).dt.month
df_test['month']=pd.to_datetime(df_test['trans_date_trans_time']).dt.month
ax=sns.histplot(data=df_train, x="month", hue="is_fraud", common_norm=False,stat='percent',multiple='dodge')
ax.set_ylabel('Percentage')
ax.set_xlabel('Month')
plt.xticks(np.arange(1,13,1))
ax.set_xticklabels(["Jan","Feb","Mar","Apr","May","Jun","Jul",'Aug','Sep','Oct','Nov','Dec'])
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

df_fraud = df_train[df_train['is_fraud'] == 1]  # Keep only fraud transactions

ax = sns.histplot(data=df_fraud, x="month", common_norm=False, stat='percent', multiple='dodge')

ax.set_ylabel('Percentage')
ax.set_xlabel('Month')

plt.xticks(np.arange(1, 13, 1), ["Jan", "Feb", "Mar", "Apr", "May", "Jun",
                                 "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"])

plt.title('Fraud Transactions by Month')
plt.show()

df_train.head()

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np

X_train = df_train.drop(columns=['trans_date_trans_time','day','hour','is_fraud','month']).copy()
y_train = df_train['is_fraud'].copy()

X_test = df_test.drop(columns=['trans_date_trans_time','day','hour','is_fraud','month']).copy()
y_test = df_test['is_fraud'].copy()

#Feature Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)

X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)

class FraudDataset(Dataset):
    def __init__(self, features, labels):
        self.features = features
        self.labels = labels
    def __len__(self):
        return len(self.features)
    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

train_dataset = FraudDataset(X_train_tensor, y_train_tensor)
test_dataset = FraudDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=512, pin_memory=True)

class BinaryClassifier(nn.Module):
    def __init__(self, input_dim):
        super(BinaryClassifier, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(32, 1)
        )
    def forward(self, x):
        return self.model(x)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BinaryClassifier(X_train_tensor.shape[1]).to(device)

criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
# Training Loop
epochs = 20
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for features, labels in train_loader:
        features, labels = features.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(features)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    avg_loss = total_loss / len(train_loader)
    print(f"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}")

#  Evaluation
model.eval()
correct = 0
total = 0
with torch.no_grad():
    for features, labels in test_loader:
        features, labels = features.to(device), labels.to(device)
        outputs = model(features)
        predicted = torch.sigmoid(outputs)
        predicted = (predicted > 0.5).float()
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
accuracy = correct / total
print(f"Test Accuracy: {accuracy:.4f}")

from sklearn.metrics import precision_score, recall_score, f1_score

model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for features, labels in test_loader:
        features = features.to(device)
        outputs = model(features)
        preds = torch.sigmoid(outputs).cpu().numpy()
        all_preds.extend(preds)
        all_labels.extend(labels.numpy())

# Convert predictions to binary labels
# Ensure all_preds is a flat list or array before converting to binary
all_preds_flat = [p for sublist in all_preds for p in (sublist if isinstance(sublist, (list, np.ndarray)) else [sublist])]
binary_preds = [1 if p > 0.5 else 0 for p in all_preds_flat]

# Ensure all_labels is a flat list or array
all_labels_flat = [l for sublist in all_labels for l in (sublist if isinstance(sublist, (list, np.ndarray)) else [sublist])]

# Calculate metrics
precision = precision_score(all_labels_flat, binary_preds)
recall = recall_score(all_labels_flat, binary_preds)
f1 = f1_score(all_labels_flat, binary_preds)

print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")
print(f"F1 Score:  {f1:.4f}")

amt_saved = df_test[(binary_preds==y_test) & (df_test.is_fraud==1)].amt.sum()

amt_saved

fraud_amt = df_test[df_test.is_fraud==1].amt.sum()

fraud_amt

saved_percentage=amt_saved/fraud_amt*100

print(f'Using this model saves {saved_percentage} % of fraud money')

#dly

import gradio as gr
import torch
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import pandas as pd
import joblib

# Load the model components
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Assuming your model and scaler are saved like this after training
# torch.save(model.state_dict(), "fraud_model.pth")
# joblib.dump(scaler, "scaler.pkl")
# joblib.dump(encoder, "encoder.pkl")

torch.save(model.state_dict(), "fraud_model.pth")
joblib.dump(scaler, "scaler.pkl")
joblib.dump(encoder, "encoder.pkl")

class BinaryClassifier(torch.nn.Module):
    def __init__(self, input_dim):
        super(BinaryClassifier, self).__init__()
        self.model = torch.nn.Sequential(
            torch.nn.Linear(input_dim, 128),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(128, 64),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(64, 32),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(32, 1)
        )
    def forward(self, x):
        return self.model(x)

# Load scaler and encoders
scaler = joblib.load("scaler.pkl")
encoder = joblib.load("encoder.pkl")

# Instantiate and load model
input_dim = scaler.transform(np.zeros((1, scaler.mean_.shape[0]))).shape[1]
model = BinaryClassifier(input_dim)
model.load_state_dict(torch.load("fraud_model.pth", map_location=device))
model.to(device)
model.eval()

# Define prediction function
def predict_fraud(category, gender, amt, hour, day, month):
    # Preprocessing
    gender = 1 if gender == "M" else 0
    is_amt_greater_than_200 = 1 if amt >= 200 else 0
    time_category = "night" if (hour >= 22 or hour <= 3) else "day"
    category_day = "cat1" if (day == 0 or day >= 5) else "cat2"

    # One-hot encode category features
    # Create dummy dataframe for transformation
    dummy_df = pd.DataFrame([[category, time_category, category_day]], columns=['category', 'time_category', 'category_day'])

    # Use the same encoder fitted on the training data
    category_encoded_df = pd.DataFrame(encoder.transform(dummy_df[['category', 'time_category', 'category_day']]), columns=encoder.get_feature_names_out(['category', 'time_category', 'category_day']))


    # Extract flattened encoded arrays - Adjusting based on your original encoding steps
    # You had separate encoding for category, time_category, and category_day
    # Let's re-create those logic here or ensure the single encoder handles them
    # If you used one encoder for all, the above should work.
    # If you used separate encoders, you'd need to load/re-fit them here.

    # Assuming the loaded 'encoder' was fitted on a DataFrame containing 'category', 'time_category', 'category_day'

    # Construct the full feature vector based on how you structured X_train
    # Need to ensure the order matches X_train columns

    # The original X_train dropped several columns and then concatenated one-hot encoded columns.
    # We need to replicate the exact input feature structure here.
    # The original columns before dropping and encoding were:
    # 'trans_date_trans_time', 'amt', 'is_fraud', 'first', 'last', 'gender', 'street', 'city', 'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'dob', 'unix_time', 'merch_lat', 'merch_long', 'is_amt_greater_than_200', 'hour', 'day', 'month', 'time_category', 'category_day'

    # Columns in X_train after preprocessing:
    # 'amt', 'gender', 'lat', 'long', 'merch_lat', 'merch_long', 'category_shopping_net', 'category_shopping_pos', 'category_others', 'category_grocery_pos', 'category_misc_net', 'is_amt_greater_than_200', 'time_category_night', 'category_day' (which was category_day_cat2)

    # Let's reconstruct the input array based on this structure
    # The order of encoded columns from the encoder matters. We need to know the order it produced during fit.
    # Assuming the encoder was fitted on df_train[['category', 'time_category', 'category_day']]

    # Let's manually reconstruct the encoded columns based on the original encoding logic if a single encoder isn't sufficient/matching

    # Re-doing the specific encoding logic for clarity based on original notebook steps
    # Step 1: category encoding
    category_encoder = OneHotEncoder(drop='first', sparse_output=False)
    category_encoder.fit(df_train[['category']]) # Fit on training data column
    category_encoded = category_encoder.transform([[category]])

    # Step 2: time_category encoding
    time_cat_encoder = OneHotEncoder(drop='first', sparse_output=False)
    time_cat_encoder.fit(df_train[['time_category']]) # Fit on training data column
    time_cat_encoded = time_cat_encoder.transform([[time_category]])

    # Step 3: category_day encoding
    day_cat_encoder = OneHotEncoder(drop='first', sparse_output=False)
    day_cat_encoder.fit(df_train[['category_day']]) # Fit on training data column
    day_cat_encoded = day_cat_encoder.transform([[category_day]])


    # Now, construct the input_features array matching the order of X_train columns
    # X_train columns order: 'amt', 'gender', 'lat', 'long', 'merch_lat', 'merch_long',
    # category encoded columns..., 'is_amt_greater_than_200',
    # time_category encoded columns..., 'category_day' (cat2)

    # We need placeholder values for 'lat', 'long', 'merch_lat', 'merch_long' since they are not inputs
    # Let's use the mean from the training data if available or just zeros/some default
    # For a live demo, ideally, these would be inputs or derived. Using zeros for now.
    lat, long, merch_lat, merch_long = 0, 0, 0, 0 # Placeholder values

    # Ensure the order of encoded columns matches X_train's column order after concat
    # Based on your code, X_train columns looked like:
    # ['amt', 'gender', 'lat', 'long', 'merch_lat', 'merch_long',
    # 'category_grocery_pos', 'category_misc_net', 'category_others', 'category_shopping_net', 'category_shopping_pos',
    # 'is_amt_greater_than_200',
    # 'time_category_night',
    # 'category_day_cat2']

    # Let's verify the order of columns generated by category_encoder, time_cat_encoder, day_cat_encoder
    category_cols = category_encoder.get_feature_names_out(['category'])
    time_cat_cols = time_cat_encoder.get_feature_names_out(['time_category'])
    day_cat_cols = day_cat_encoder.get_feature_names_out(['category_day']) # Should be ['category_day_cat2']


    # Manually construct the input row based on the expected X_train column order
    input_row = [amt, gender, lat, long, merch_lat, merch_long]

    # Add category encoded values in the correct order
    # The order depends on the output of category_encoder.get_feature_names_out()
    # We need to ensure the columns we pick from category_encoded match the order in X_train
    # From your X_train head/info, the order seems to be 'category_grocery_pos', 'category_misc_net', 'category_others', 'category_shopping_net', 'category_shopping_pos'
    # Let's map the encoded values to this specific order
    category_encoded_dict = dict(zip(category_encoder.get_feature_names_out(), category_encoded[0]))
    input_row.append(category_encoded_dict.get('category_grocery_pos', 0.0))
    input_row.append(category_encoded_dict.get('category_misc_net', 0.0))
    input_row.append(category_encoded_dict.get('category_others', 0.0))
    input_row.append(category_encoded_dict.get('category_shopping_net', 0.0))
    input_row.append(category_encoded_dict.get('category_shopping_pos', 0.0))

    # Add is_amt_greater_than_200
    input_row.append(is_amt_greater_than_200)

    # Add time_category encoded value ('time_category_night')
    time_cat_encoded_dict = dict(zip(time_cat_encoder.get_feature_names_out(), time_cat_encoded[0]))
    input_row.append(time_cat_encoded_dict.get('time_category_night', 0.0))

    # Add category_day encoded value ('category_day_cat2')
    day_cat_encoded_dict = dict(zip(day_cat_encoder.get_feature_names_out(), day_cat_encoded[0]))
    input_row.append(day_cat_encoded_dict.get('category_day_cat2', 0.0))

    input_features = np.array(input_row)


    # Feature scaling
    input_scaled = scaler.transform([input_features])
    input_tensor = torch.tensor(input_scaled, dtype=torch.float32).to(device)

    with torch.no_grad():
        output = torch.sigmoid(model(input_tensor))
        prediction = output.item()

    return {"Fraud Probability": round(prediction, 4), "Prediction": "Fraud" if prediction > 0.5 else "Not Fraud"}

# Gradio UI
category_options = ['grocery_pos', 'shopping_net', 'misc_net', 'shopping_pos', 'gas_transport', 'others']
interface = gr.Interface(
    fn=predict_fraud,
    inputs=[
        gr.Dropdown(category_options, label="Category"),
        gr.Radio(["M", "F"], label="Gender"),
        gr.Number(label="Transaction Amount (USD)"),
        gr.Slider(0, 23, step=1, label="Hour of Transaction"),
        gr.Slider(0, 6, step=1, label="Day of Week (0=Mon, 6=Sun)"),
        gr.Slider(1, 12, step=1, label="Month")
    ],
    outputs=[
        gr.Number(label="Fraud Probability"),
        gr.Text(label="Prediction")
    ],
    title="Fraud Detection Model",
    description="Enter transaction details to predict if it's fraudulent."
)
!pip install gradio

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df_train = pd.read_csv('fraudTrain.csv', index_col=0)
df_test = pd.read_csv('fraudTest.csv',index_col=0)

df_train.head()

df_train.columns

row,col=df_train.shape
print(f'The dataset has {row} rows and {col} columns.')

print(df_train.describe())

df_train.info()

fraud_percentage=len(df_train[df_train['is_fraud']==1])/len(df_train)*100
non_fraud_percentage=100-fraud_percentage
print(f'The percentage of fraud is {fraud_percentage} % and the percentage of non-fraud is {non_fraud_percentage} %')
print('This shows how imbalanced is the dataset')

df_train.isna().sum()

len(df_train)==len(df_train.drop_duplicates())

df_train.head()

df_train['cc_num'].value_counts()

df_train[df_train['is_fraud']==1].cc_num.value_counts()

df_train.drop(columns=['cc_num'],inplace=True)
df_test.drop(columns=['cc_num'],inplace=True)

df_train.merchant.value_counts()

df_train[df_train['is_fraud']==1].merchant.value_counts().head(50)

df_train.drop(columns=['merchant'],inplace=True)
df_test.drop(columns=['merchant'],inplace=True)

df_train.category.value_counts()

df_train[df_train['is_fraud']==1].category.value_counts()

(1743+1713+915+843+618)/(df_train[df_train['is_fraud']==1].category.value_counts().sum())*100

df_train.groupby('category')['amt'].sum().sort_values(ascending=False)

(14460822.38+9307993.61+8625149.68+8351732.29+7173928.11)/(df_train.groupby('category')['amt'].sum().sum())*100

def convert_to_other(instance):
    if(instance in ['grocery_pos', 'shopping_net', 'misc_net','shopping_pos', 'gas_transport']):
        return instance
    else :
        return 'others'
df_train['category']=df_train['category'].apply(convert_to_other)
df_test['category']=df_test['category'].apply(convert_to_other)
df_train.category.value_counts()

df_train.head()

from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(drop='first', sparse_output=False)
# drop='first' removes one column to avoid redundancy which also remove multicollinearity
# if we don't remove the input feature will have depencies. sum of values in the col will be 1 which can effect the performance

encoded_array = encoder.fit_transform(df_train[['category']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category']))

df_train = pd.concat([df_train.drop(columns=['category']), encoded_df], axis=1)

encoded_array = encoder.transform(df_test[['category']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category']))

df_test = pd.concat([df_test.drop(columns=['category']), encoded_df], axis=1)

df_train.drop(columns=['first','last'],inplace=True)
df_test.drop(columns=['first','last'],inplace=True)

def M_1_F_0(instance):
    if(instance=='M'):
        return 1;
    return 0;

df_train['gender']=df_train['gender'].apply(M_1_F_0)
df_test['gender']=df_test['gender'].apply(M_1_F_0)

df_train.street.value_counts()

df_train[df_train.is_fraud==1].city.value_counts()

df_train['zip'].value_counts()

df_train.drop(columns=['street','city','state','zip','dob','trans_num','city_pop'],inplace=True)
df_test.drop(columns=['street','city','state','zip','dob','trans_num','city_pop'],inplace=True)
df_train.head()

df_train.drop(columns=['unix_time'],inplace=True)
df_test.drop(columns=['unix_time'],inplace=True)

df_train.head()

df_train.job.value_counts()

df_train[df_train.is_fraud==1].job.value_counts()

(62+56+53+51+50)/(len(df_train[df_train.is_fraud==1]))*100

df_train.drop(columns=['job'],inplace=True)
df_test.drop(columns=['job'],inplace=True)
df_train.head()

ax=sns.histplot(x='amt',data=df_train[df_train.amt<=1000],hue='is_fraud',stat='percent',multiple='dodge',common_norm=False,bins=25)
ax.set_ylabel('Percentage in Each Type')
ax.set_xlabel('Transaction Amount in USD')
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

df_train[(df_train.amt>=200)& (df_train.is_fraud==1)].amt.sum()/df_train[df_train.is_fraud==1].amt.sum()*100

def is_greater_than_200(amt):
    if(amt>=200):
        return 1;
    return 0;
df_train['is_amt_greater_than_200']=df_train.amt.apply(is_greater_than_200)
df_test['is_amt_greater_than_200']=df_test.amt.apply(is_greater_than_200)

split_sum = df_train[df_train.is_fraud==1].groupby('is_amt_greater_than_200')['amt'].sum()

plt.bar(split_sum.index.astype(str), split_sum.values, color=['blue', 'orange'])

df_train['hour']=pd.to_datetime(df_train['trans_date_trans_time']).dt.hour
df_test['hour']=pd.to_datetime(df_test['trans_date_trans_time']).dt.hour
ax=sns.histplot(data=df_train, x="hour", hue="is_fraud", common_norm=False,stat='percent',multiple='dodge')
ax.set_ylabel('Percentage')
ax.set_xlabel('Time (Hour) in a Day')
plt.xticks(np.arange(0,24,1))
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

df_train[(df_train.is_fraud == 1) & ((df_train.hour <= 3) | (df_train.hour >= 22))]['amt'].sum()/(df_train[df_train.is_fraud==1].amt.sum())*100

def time_category(hour):
    if hour>=22 or hour<=3 :
        return "night";
    else :
        return "day"

df_train['time_category']= df_train['hour'].apply(time_category)
df_test['time_category']= df_test['hour'].apply(time_category)

df_train.time_category.value_counts()

df_train.info()

encoder = OneHotEncoder(drop='first', sparse_output=False)
encoded_array = encoder.fit_transform(df_train[['time_category']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['time_category']))

df_train = pd.concat([df_train.drop(columns=['time_category']), encoded_df], axis=1)

encoded_array = encoder.transform(df_test[['time_category']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['time_category']))

df_test = pd.concat([df_test.drop(columns=['time_category']), encoded_df], axis=1)

df_train.info()

df_train.head()

df_train['day']=pd.to_datetime(df_train['trans_date_trans_time']).dt.dayofweek
df_test['day']=pd.to_datetime(df_test['trans_date_trans_time']).dt.dayofweek
ax=sns.histplot(data=df_train, x="day", hue="is_fraud", common_norm=False,stat='percent',multiple='dodge')
#ax.set_xticklabels(['',"Mon","Tue","Wed","Thu","Fri","Sat","Sun"])
plt.xticks(np.arange(7), ["Mon","Tue","Wed","Thu","Fri","Sat","Sun"]) #set the xtick labels
ax.set_ylabel('Percentage')
ax.set_xlabel('Day of Week')
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

df_train[(df_train.is_fraud == 1) & ((df_train.day >= 5)|(df_train.day==0))]['amt'].sum() / df_train[df_train.is_fraud == 1]['amt'].sum()*100

def category_day(day):
    if(day==0 or day>=5):
        return "cat1";
    else:
        return "cat2"
df_train['category_day']=df_train['day'].apply(category_day)
df_test['category_day']=df_test['day'].apply(category_day)

encoded_array = encoder.fit_transform(df_train[['category_day']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category_day']))

df_train = pd.concat([df_train.drop(columns=['category_day']), encoded_df], axis=1)

encoded_array = encoder.transform(df_test[['category_day']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category_day']))

df_test = pd.concat([df_test.drop(columns=['category_day']), encoded_df], axis=1)

df_test['category_day']=df_test['category_day_cat2']
df_test.drop(columns=['category_day_cat2'],inplace=True)

df_train['category_day']=df_train['category_day_cat2']
df_train.drop(columns=['category_day_cat2'],inplace=True)

df_train.category_day.value_counts()

df_fraud = df_train[df_train['is_fraud'] == 1]  # Keep only fraud transactions

ax = sns.histplot(data=df_fraud, x="day", common_norm=False, stat='percent', multiple='dodge')

ax.set_xticklabels(["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"])
ax.set_ylabel('Percentage')
ax.set_xlabel('Day of Week')
plt.title('Fraud Transactions by Day of the Week')
plt.show()

df_train['month']=pd.to_datetime(df_train['trans_date_trans_time']).dt.month
df_test['month']=pd.to_datetime(df_test['trans_date_trans_time']).dt.month
ax=sns.histplot(data=df_train, x="month", hue="is_fraud", common_norm=False,stat='percent',multiple='dodge')
ax.set_ylabel('Percentage')
ax.set_xlabel('Month')
plt.xticks(np.arange(1,13,1))
ax.set_xticklabels(["Jan","Feb","Mar","Apr","May","Jun","Jul",'Aug','Sep','Oct','Nov','Dec'])
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

df_fraud = df_train[df_train['is_fraud'] == 1]  # Keep only fraud transactions

ax = sns.histplot(data=df_fraud, x="month", common_norm=False, stat='percent', multiple='dodge')

ax.set_ylabel('Percentage')
ax.set_xlabel('Month')

plt.xticks(np.arange(1, 13, 1), ["Jan", "Feb", "Mar", "Apr", "May", "Jun",
                                 "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"])

plt.title('Fraud Transactions by Month')
plt.show()

df_train.head()

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np

X_train = df_train.drop(columns=['trans_date_trans_time','day','hour','is_fraud','month']).copy()
y_train = df_train['is_fraud'].copy()

X_test = df_test.drop(columns=['trans_date_trans_time','day','hour','is_fraud','month']).copy()
y_test = df_test['is_fraud'].copy()

#Feature Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)

X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)

class FraudDataset(Dataset):
    def __init__(self, features, labels):
        self.features = features
        self.labels = labels
    def __len__(self):
        return len(self.features)
    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

train_dataset = FraudDataset(X_train_tensor, y_train_tensor)
test_dataset = FraudDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=512, pin_memory=True)

class BinaryClassifier(nn.Module):
    def __init__(self, input_dim):
        super(BinaryClassifier, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(32, 1)
        )
    def forward(self, x):
        return self.model(x)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BinaryClassifier(X_train_tensor.shape[1]).to(device)

criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
# Training Loop
epochs = 20
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for features, labels in train_loader:
        features, labels = features.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(features)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    avg_loss = total_loss / len(train_loader)
    print(f"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}")

#  Evaluation
model.eval()
correct = 0
total = 0
with torch.no_grad():
    for features, labels in test_loader:
        features, labels = features.to(device), labels.to(device)
        outputs = model(features)
        predicted = torch.sigmoid(outputs)
        predicted = (predicted > 0.5).float()
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
accuracy = correct / total
print(f"Test Accuracy: {accuracy:.4f}")

from sklearn.metrics import precision_score, recall_score, f1_score

model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for features, labels in test_loader:
        features = features.to(device)
        outputs = model(features)
        preds = torch.sigmoid(outputs).cpu().numpy()
        all_preds.extend(preds)
        all_labels.extend(labels.numpy())

# Convert predictions to binary labels
# Ensure all_preds is a flat list or array before converting to binary
all_preds_flat = [p for sublist in all_preds for p in (sublist if isinstance(sublist, (list, np.ndarray)) else [sublist])]
binary_preds = [1 if p > 0.5 else 0 for p in all_preds_flat]

# Ensure all_labels is a flat list or array
all_labels_flat = [l for sublist in all_labels for l in (sublist if isinstance(sublist, (list, np.ndarray)) else [sublist])]

# Calculate metrics
precision = precision_score(all_labels_flat, binary_preds)
recall = recall_score(all_labels_flat, binary_preds)
f1 = f1_score(all_labels_flat, binary_preds)

print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")
print(f"F1 Score:  {f1:.4f}")

amt_saved = df_test[(binary_preds==y_test) & (df_test.is_fraud==1)].amt.sum()

amt_saved

fraud_amt = df_test[df_test.is_fraud==1].amt.sum()

fraud_amt

saved_percentage=amt_saved/fraud_amt*100

print(f'Using this model saves {saved_percentage} % of fraud money')

#dly

import gradio as gr
import torch
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import pandas as pd
import joblib

# Load the model components
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Assuming your model and scaler are saved like this after training
# torch.save(model.state_dict(), "fraud_model.pth")
# joblib.dump(scaler, "scaler.pkl")
# joblib.dump(encoder, "encoder.pkl")

torch.save(model.state_dict(), "fraud_model.pth")
joblib.dump(scaler, "scaler.pkl")
joblib.dump(encoder, "encoder.pkl")

class BinaryClassifier(torch.nn.Module):
    def __init__(self, input_dim):
        super(BinaryClassifier, self).__init__()
        self.model = torch.nn.Sequential(
            torch.nn.Linear(input_dim, 128),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(128, 64),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(64, 32),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(32, 1)
        )
    def forward(self, x):
        return self.model(x)

# Load scaler and encoders
scaler = joblib.load("scaler.pkl")
encoder = joblib.load("encoder.pkl")

# Instantiate and load model
# Need to fit the encoder on a small dummy dataset with all possible categories
# to ensure it has the correct mapping for all inputs in the Gradio interface.
# Or better yet, load the encoder that was fitted on the training data if it includes all categories.
# Since the original code fits the encoder *during* the notebook execution and then re-fits,
# the best approach for the Gradio app is to re-fit the encoders on relevant columns from df_train.

# Re-fit the specific encoders used in the notebook
category_encoder = OneHotEncoder(drop='first', sparse_output=False)
category_encoder.fit(df_train[['category']].astype(str)) # Fit on training data column

time_cat_encoder = OneHotEncoder(drop='first', sparse_output=False)
time_cat_encoder.fit(df_train[['time_category']].astype(str)) # Fit on training data column

day_cat_encoder = OneHotEncoder(drop='first', sparse_output=False)
day_cat_encoder.fit(df_train[['category_day']].astype(str)) # Fit on training data column

# Determine the input dimension based on the final X_train columns structure
# This is tricky without knowing the exact column order after all transformations.
# A more robust way is to save the list of X_train columns or a sample of X_train.
# Let's manually define the expected input columns based on the preprocessing steps:
# 'amt', 'gender', 'lat', 'long', 'merch_lat', 'merch_long',
# category encoded columns..., 'is_amt_greater_than_200',
# time_category encoded columns..., 'category_day' (cat2)

# Let's determine the number of columns from the encoders' output
num_category_cols = category_encoder.get_feature_names_out(['category']).shape[0]
num_time_cat_cols = time_cat_encoder.get_feature_names_out(['time_category']).shape[0]
num_day_cat_cols = day_cat_encoder.get_feature_names_out(['category_day']).shape[0]

# Base columns not one-hot encoded (amt, gender, lat, long, merch_lat, merch_long, is_amt_greater_than_200)
# lat, long, merch_lat, merch_long were placeholders. Let's count based on X_train columns:
# X_train columns: 'amt', 'gender', 'lat', 'long', 'merch_lat', 'merch_long',
# 'category_grocery_pos', 'category_misc_net', 'category_others', 'category_shopping_net', 'category_shopping_pos',
# 'is_amt_greater_than_200',
# 'time_category_night',
# 'category_day'

# There are 7 non-OHE columns + 5 category OHE + 1 time_category OHE + 1 category_day OHE
# Total expected columns = 7 + num_category_cols + num_time_cat_cols + num_day_cat_cols
# Assuming drop='first' and the specific categories in df_train:
# category_encoder will produce columns for 'grocery_pos', 'misc_net', 'others', 'shopping_net', 'shopping_pos' (5 columns)
# time_cat_encoder will produce column for 'night' (1 column)
# day_cat_encoder will produce column for 'cat2' (1 column)
# So, 7 + 5 + 1 + 1 = 14 columns.
# Let's verify X_train_tensor.shape[1] which is 14. This matches.

input_dim = X_train_tensor.shape[1]
model = BinaryClassifier(input_dim)
model.load_state_dict(torch.load("fraud_model.pth", map_location=device))
model.to(device)
model.eval()

# Define prediction function
def predict_fraud(category, gender, amt, hour, day, month):
    # Preprocessing
    gender = 1 if gender == "M" else 0
    is_amt_greater_than_200 = 1 if amt >= 200 else 0
    time_category_val = "night" if (hour >= 22 or hour <= 3) else "day"
    category_day_val = "cat1" if (day == 0 or day >= 5) else "cat2"

    # One-hot encode category features using the re-fitted encoders
    # Need to pass input as a 2D array or list of lists
    category_encoded = category_encoder.transform([[category]]).flatten()
    time_cat_encoded = time_cat_encoder.transform([[time_category_val]]).flatten()
    cat_day_encoded = day_cat_encoder.transform([[category_day_val]]).flatten()

    # Placeholder values for lat, long, merch_lat, merch_long (if not provided as input)
    lat, long, merch_lat, merch_long = 0, 0, 0, 0

    # Construct the full feature vector matching the order of X_train columns
    # Order: 'amt', 'gender', 'lat', 'long', 'merch_lat', 'merch_long',
    # category_encoded_cols..., 'is_amt_greater_than_200',
    # time_category_encoded_cols..., 'category_day_cat2'

    # Get the column names from the category encoder to match the order
    category_cols_ordered = category_encoder.get_feature_names_out(['category'])
    time_cat_cols_ordered = time_cat_encoder.get_feature_names_out(['time_category'])
    day_cat_cols_ordered = day_cat_encoder.get_feature_names_out(['category_day']) # Should be ['category_day_cat2']

    # Create dictionaries for easy lookup of encoded values
    category_encoded_dict = dict(zip(category_cols_ordered, category_encoded))
    time_cat_encoded_dict = dict(zip(time_cat_cols_ordered, time_cat_encoded))
    day_cat_encoded_dict = dict(zip(day_cat_cols_ordered, cat_day_encoded))


    # Manually construct the input row based on the expected X_train column order
    # This order was determined by looking at your X_train structure and preprocessing steps.
    # ['amt', 'gender', 'lat', 'long', 'merch_lat', 'merch_long',
    # 'category_grocery_pos', 'category_misc_net', 'category_others', 'category_shopping_net', 'category_shopping_pos',
    # 'is_amt_greater_than_200',
    # 'time_category_night',
    # 'category_day'] # This last one was 'category_day_cat2' after dropping 'category_day_cat1'

    input_row = [amt, gender, lat, long, merch_lat, merch_long]

    # Add category encoded values in the order they appear in X_train
    input_row.append(category_encoded_dict.get('category_grocery_pos', 0.0))
    input_row.append(category_encoded_dict.get('category_misc_net', 0.0))
    input_row.append(category_encoded_dict.get('category_others', 0.0))
    input_row.append(category_encoded_dict.get('category_shopping_net', 0.0))
    input_row.append(category_encoded_dict.get('category_shopping_pos', 0.0))

    # Add is_amt_greater_than_200
    input_row.append(is_amt_greater_than_200)

    # Add time_category encoded value ('time_category_night')
    input_row.append(time_cat_encoded_dict.get('time_category_night', 0.0))

    # Add category_day encoded value ('category_day_cat2')
    input_row.append(day_cat_encoded_dict.get('category_day_cat2', 0.0))


    input_features = np.array(input_row).astype(np.float64) # Ensure float type for scaler

    # Feature scaling
    input_scaled = scaler.transform([input_features])
    input_tensor = torch.tensor(input_scaled, dtype=torch.float32).to(device)

    with torch.no_grad():
        output = torch.sigmoid(model(input_tensor))
        prediction = output.item()

    return {"Fraud Probability": round(prediction, 4), "Prediction": "Fraud" if prediction > 0.5 else "Not Fraud"}

# Gradio UI
category_options = ['grocery_pos', 'shopping_net', 'misc_net', 'shopping_pos', 'gas_transport', 'others']
interface = gr.Interface(
    fn=predict_fraud,
    inputs=[
        gr.Dropdown(category_options, label="Category"),
        gr.Radio(["M", "F"], label="Gender"),
        gr.Number(label="Transaction Amount (USD)"),
        gr.Slider(0, 23, step=1, label="Hour of Transaction"),
        gr.Slider(0, 6, step=1, label="Day of Week (0=Mon, 6=Sun)"),
        gr.Slider(1, 12, step=1, label="Month")
    ],
    outputs=[
        gr.Number(label="Fraud Probability"),
        gr.Text(label="Prediction")
    ],
    title="Fraud Detection Model",
    description="Enter transaction details to predict if it's fraudulent."
)

interface.launch()
df_train.category.value_counts()

df_train.head()

from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(drop='first', sparse_output=False)
# drop='first' removes one column to avoid redundancy which also remove multicollinearity
# if we don't remove the input feature will have depencies. sum of values in the col will be 1 which can effect the performance

encoded_array = encoder.fit_transform(df_train[['category']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category']))

df_train = pd.concat([df_train.drop(columns=['category']), encoded_df], axis=1)

encoded_array = encoder.transform(df_test[['category']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category']))

df_test = pd.concat([df_test.drop(columns=['category']), encoded_df], axis=1)

df_train.drop(columns=['first','last'],inplace=True)
df_test.drop(columns=['first','last'],inplace=True)
!pip install gradio

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df_train = pd.read_csv('fraudTrain.csv', index_col=0)
df_test = pd.read_csv('fraudTest.csv',index_col=0)

df_train.head()

df_train.columns

row,col=df_train.shape
print(f'The dataset has {row} rows and {col} columns.')

print(df_train.describe())

df_train.info()

fraud_percentage=len(df_train[df_train['is_fraud']==1])/len(df_train)*100
non_fraud_percentage=100-fraud_percentage
print(f'The percentage of fraud is {fraud_percentage} % and the percentage of non-fraud is {non_fraud_percentage} %')
print('This shows how imbalanced is the dataset')

df_train.isna().sum()

len(df_train)==len(df_train.drop_duplicates())

df_train.head()

df_train['cc_num'].value_counts()

df_train[df_train['is_fraud']==1].cc_num.value_counts()

df_train.drop(columns=['cc_num'],inplace=True)
df_test.drop(columns=['cc_num'],inplace=True)

df_train.merchant.value_counts()

df_train[df_train['is_fraud']==1].merchant.value_counts().head(50)

df_train.drop(columns=['merchant'],inplace=True)
df_test.drop(columns=['merchant'],inplace=True)

df_train.category.value_counts()

df_train[df_train['is_fraud']==1].category.value_counts()

(1743+1713+915+843+618)/(df_train[df_train['is_fraud']==1].category.value_counts().sum())*100

df_train.groupby('category')['amt'].sum().sort_values(ascending=False)

(14460822.38+9307993.61+8625149.68+8351732.29+7173928.11)/(df_train.groupby('category')['amt'].sum().sum())*100

def convert_to_other(instance):
    if(instance in ['grocery_pos', 'shopping_net', 'misc_net','shopping_pos', 'gas_transport']):
        return instance
    else :
        return 'others'
df_train['category']=df_train['category'].apply(convert_to_other)
df_test['category']=df_test['category'].apply(convert_to_other)
df_train.category.value_counts()

df_train.head()

from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(drop='first', sparse_output=False)
# drop='first' removes one column to avoid redundancy which also remove multicollinearity
# if we don't remove the input feature will have depencies. sum of values in the col will be 1 which can effect the performance

encoded_array = encoder.fit_transform(df_train[['category']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category']))

df_train = pd.concat([df_train.drop(columns=['category']), encoded_df], axis=1)

encoded_array = encoder.transform(df_test[['category']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category']))

df_test = pd.concat([df_test.drop(columns=['category']), encoded_df], axis=1)

df_train.drop(columns=['first','last'],inplace=True)
df_test.drop(columns=['first','last'],inplace=True)

def M_1_F_0(instance):
    if(instance=='M'):
        return 1
    return 0

df_train['gender']=df_train['gender'].apply(M_1_F_0)
df_test['gender']=df_test['gender'].apply(M_1_F_0)

df_train.street.value_counts()

df_train[df_train.is_fraud==1].city.value_counts()

df_train['zip'].value_counts()

df_train.drop(columns=['street','city','state','zip','dob','trans_num','city_pop'],inplace=True)
df_test.drop(columns=['street','city','state','zip','dob','trans_num','city_pop'],inplace=True)
df_train.head()

df_train.drop(columns=['unix_time'],inplace=True)
df_test.drop(columns=['unix_time'],inplace=True)

df_train.head()

df_train.job.value_counts()

df_train[df_train.is_fraud==1].job.value_counts()

(62+56+53+51+50)/(len(df_train[df_train.is_fraud==1]))*100

df_train.drop(columns=['job'],inplace=True)
df_test.drop(columns=['job'],inplace=True)
df_train.head()

ax=sns.histplot(x='amt',data=df_train[df_train.amt<=1000],hue='is_fraud',stat='percent',multiple='dodge',common_norm=False,bins=25)
ax.set_ylabel('Percentage in Each Type')
ax.set_xlabel('Transaction Amount in USD')
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

df_train[(df_train.amt>=200)& (df_train.is_fraud==1)].amt.sum()/df_train[df_train.is_fraud==1].amt.sum()*100

def is_greater_than_200(amt):
    if(amt>=200):
        return 1
    return 0
df_train['is_amt_greater_than_200']=df_train.amt.apply(is_greater_than_200)
df_test['is_amt_greater_than_200']=df_test.amt.apply(is_greater_than_200)

split_sum = df_train[df_train.is_fraud==1].groupby('is_amt_greater_than_200')['amt'].sum()

plt.bar(split_sum.index.astype(str), split_sum.values, color=['blue', 'orange'])

df_train['hour']=pd.to_datetime(df_train['trans_date_trans_time']).dt.hour
df_test['hour']=pd.to_datetime(df_test['trans_date_trans_time']).dt.hour
ax=sns.histplot(data=df_train, x="hour", hue="is_fraud", common_norm=False,stat='percent',multiple='dodge')
ax.set_ylabel('Percentage')
ax.set_xlabel('Time (Hour) in a Day')
plt.xticks(np.arange(0,24,1))
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

df_train[(df_train.is_fraud == 1) & ((df_train.hour <= 3) | (df_train.hour >= 22))]['amt'].sum()/(df_train[df_train.is_fraud==1].amt.sum())*100

def time_category(hour):
    if hour>=22 or hour<=3 :
        return "night"
    else :
        return "day"

df_train['time_category']= df_train['hour'].apply(time_category)
df_test['time_category']= df_test['hour'].apply(time_category)

df_train.time_category.value_counts()

df_train.info()

encoder = OneHotEncoder(drop='first', sparse_output=False)
encoded_array = encoder.fit_transform(df_train[['time_category']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['time_category']))

df_train = pd.concat([df_train.drop(columns=['time_category']), encoded_df], axis=1)

encoded_array = encoder.transform(df_test[['time_category']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['time_category']))

df_test = pd.concat([df_test.drop(columns=['time_category']), encoded_df], axis=1)

df_train.info()

df_train.head()

df_train['day']=pd.to_datetime(df_train['trans_date_trans_time']).dt.dayofweek
df_test['day']=pd.to_datetime(df_test['trans_date_trans_time']).dt.dayofweek
ax=sns.histplot(data=df_train, x="day", hue="is_fraud", common_norm=False,stat='percent',multiple='dodge')
#ax.set_xticklabels(['',"Mon","Tue","Wed","Thu","Fri","Sat","Sun"])
plt.xticks(np.arange(7), ["Mon","Tue","Wed","Thu","Fri","Sat","Sun"]) #set the xtick labels
ax.set_ylabel('Percentage')
ax.set_xlabel('Day of Week')
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

df_train[(df_train.is_fraud == 1) & ((df_train.day >= 5)|(df_train.day==0))]['amt'].sum() / df_train[df_train.is_fraud == 1]['amt'].sum()*100

def category_day(day):
    if(day==0 or day>=5):
        return "cat1"
    else:
        return "cat2"
df_train['category_day']=df_train['day'].apply(category_day)
df_test['category_day']=df_test['day'].apply(category_day)

encoded_array = encoder.fit_transform(df_train[['category_day']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category_day']))

df_train = pd.concat([df_train.drop(columns=['category_day']), encoded_df], axis=1)

encoded_array = encoder.transform(df_test[['category_day']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category_day']))

df_test = pd.concat([df_test.drop(columns=['category_day']), encoded_df], axis=1)

df_test['category_day']=df_test['category_day_cat2']
df_test.drop(columns=['category_day_cat2'],inplace=True)

df_train['category_day']=df_train['category_day_cat2']
df_train.drop(columns=['category_day_cat2'],inplace=True)

df_train.category_day.value_counts()

df_fraud = df_train[df_train['is_fraud'] == 1]  # Keep only fraud transactions

ax = sns.histplot(data=df_fraud, x="day", common_norm=False, stat='percent', multiple='dodge')

ax.set_xticklabels(["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"])
ax.set_ylabel('Percentage')
ax.set_xlabel('Day of Week')
plt.title('Fraud Transactions by Day of the Week')
plt.show()

df_train['month']=pd.to_datetime(df_train['trans_date_trans_time']).dt.month
df_test['month']=pd.to_datetime(df_test['trans_date_trans_time']).dt.month
ax=sns.histplot(data=df_train, x="month", hue="is_fraud", common_norm=False,stat='percent',multiple='dodge')
ax.set_ylabel('Percentage')
ax.set_xlabel('Month')
plt.xticks(np.arange(1,13,1))
ax.set_xticklabels(["Jan","Feb","Mar","Apr","May","Jun","Jul",'Aug','Sep','Oct','Nov','Dec'])
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

df_fraud = df_train[df_train['is_fraud'] == 1]  # Keep only fraud transactions

ax = sns.histplot(data=df_fraud, x="month", common_norm=False, stat='percent', multiple='dodge')

ax.set_ylabel('Percentage')
ax.set_xlabel('Month')

plt.xticks(np.arange(1, 13, 1), ["Jan", "Feb", "Mar", "Apr", "May", "Jun",
                                 "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"])

plt.title('Fraud Transactions by Month')
plt.show()

df_train.head()

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np

X_train = df_train.drop(columns=['trans_date_trans_time','day','hour','is_fraud','month']).copy()
y_train = df_train['is_fraud'].copy()

X_test = df_test.drop(columns=['trans_date_trans_time','day','hour','is_fraud','month']).copy()
y_test = df_test['is_fraud'].copy()

#Feature Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)

X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)

class FraudDataset(Dataset):
    def __init__(self, features, labels):
        self.features = features
        self.labels = labels
    def __len__(self):
        return len(self.features)
    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

train_dataset = FraudDataset(X_train_tensor, y_train_tensor)
test_dataset = FraudDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=512, pin_memory=True)

class BinaryClassifier(nn.Module):
    def __init__(self, input_dim):
        super(BinaryClassifier, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(32, 1)
        )
    def forward(self, x):
        return self.model(x)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BinaryClassifier(X_train_tensor.shape[1]).to(device)

criterion = nn.BCEWithItsLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
# Training Loop
epochs = 20
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for features, labels in train_loader:
        features, labels = features.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(features)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    avg_loss = total_loss / len(train_loader)
    print(f"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}")

#  Evaluation
model.eval()
correct = 0
total = 0
with torch.no_grad():
    for features, labels in test_loader:
        features, labels = features.to(device), labels.to(device)
        outputs = model(features)
        predicted = torch.sigmoid(outputs)
        predicted = (predicted > 0.5).float()
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
accuracy = correct / total
print(f"Test Accuracy: {accuracy:.4f}")

from sklearn.metrics import precision_score, recall_score, f1_score

model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for features, labels in test_loader:
        features = features.to(device)
        outputs = model(features)
        preds = torch.sigmoid(outputs).cpu().numpy()
        all_preds.extend(preds)
        all_labels.extend(labels.numpy())

# Convert predictions to binary labels
# Ensure all_preds is a flat list or array before converting to binary
all_preds_flat = [p for sublist in all_preds for p in (sublist if isinstance(sublist, (list, np.ndarray)) else [sublist])]
binary_preds = [1 if p > 0.5 else 0 for p in all_preds_flat]

# Ensure all_labels is a flat list or array
all_labels_flat = [l for sublist in all_labels for l in (sublist if isinstance(sublist, (list, np.ndarray)) else [sublist])]

# Calculate metrics
precision = precision_score(all_labels_flat, binary_preds)
recall = recall_score(all_labels_flat, binary_preds)
f1 = f1_score(all_labels_flat, binary_preds)

print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")
print(f"F1 Score:  {f1:.4f}")

amt_saved = df_test[(binary_preds==y_test) & (df_test.is_fraud==1)].amt.sum()

amt_saved

fraud_amt = df_test[df_test.is_fraud==1].amt.sum()

fraud_amt

saved_percentage=amt_saved/fraud_amt*100

print(f'Using this model saves {saved_percentage} % of fraud money')

#dly

import gradio as gr
import torch
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import pandas as pd
import joblib

# Load the model components
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Assuming your model and scaler are saved like this after training
# torch.save(model.state_dict(), "fraud_model.pth")
# joblib.dump(scaler, "scaler.pkl")
# joblib.dump(encoder, "encoder.pkl")

torch.save(model.state_dict(), "fraud_model.pth")
# You should save the scaler and the specific encoders used for category, time_category, day_category
# during training, not the 'encoder' variable which was reused.
# Let's re-fit them for the Gradio app as done in the corrected Gradio block below.
# joblib.dump(scaler, "scaler.pkl")
# joblib.dump(category_encoder, "category_encoder.pkl")
# joblib.dump(time_cat_encoder, "time_cat_encoder.pkl")
# joblib.dump(day_cat_encoder, "day_cat_encoder.pkl")


class BinaryClassifier(torch.nn.Module):
    def __init__(self, input_dim):
        super(BinaryClassifier, self).__init__()
        self.model = torch.nn.Sequential(
            torch.nn.Linear(input_dim, 128),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(128, 64),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(64, 32),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(32, 1)
        )
    def forward(self, x):
        return self.model(x)

# Load scaler
scaler = joblib.load("scaler.pkl")

# Re-fit the specific encoders used in the notebook for prediction
# You need access to the training data or a sample with all possible values
# to fit the encoders correctly. Using df_train here assuming it's available.
# If df_train isn't available in the Gradio environment, you'd need to save and load these encoders.
if 'df_train' in locals():
    category_encoder = OneHotEncoder(drop='first', sparse_output=False)
    category_encoder.fit(df_train[['category']].astype(str))

    time_cat_encoder = OneHotEncoder(drop='first', sparse_output=False)
    time_cat_encoder.fit(df_train[['time_category']].astype(str))

    day_cat_encoder = OneHotEncoder(drop='first', sparse_output=False)
    day_cat_encoder.fit(df_train[['category_day']].astype(str))
else:
    # If df_train is not available, load pre-fitted encoders
    # category_encoder = joblib.load("category_encoder.pkl")
    # time_cat_encoder = joblib.load("time_cat_encoder.pkl")
    # day_cat_encoder = joblib.load("day_cat_encoder.pkl")
    # Placeholder/Error handling if encoders aren't available
    raise FileNotFoundError("Training data (df_train) not found. Please ensure it's available or load pre-fitted encoders.")


# Determine the input dimension based on the final X_train columns structure
# This relies on knowing the structure of X_train after preprocessing.
# If you saved X_train columns, you could load them. Otherwise, manually construct based on logic.
# Based on the manual reconstruction in the previous block, the input dimension is 14.
input_dim = 14 # This should match X_train_tensor.shape[1] from training

model = BinaryClassifier(input_dim)
model.load_state_dict(torch.load("fraud_model.pth", map_location=device))
model.to(device)
model.eval()

# Define prediction function
def predict_fraud(category, gender, amt, hour, day, month):
    # Preprocessing replicating training steps
    gender_processed = 1 if gender == "M" else 0
    is_amt_greater_than_200 = 1 if amt >= 200 else 0
    time_category_val = "night" if (hour >= 22 or hour <= 3) else "day"
    category_day_val = "cat1" if (day == 0 or day >= 5) else "cat2"

    # One-hot encode using the specific encoders
    category_encoded = category_encoder.transform([[category]]).flatten()
    time_cat_encoded = time_cat_encoder.transform([[time_category_val]]).flatten()
    cat_day_encoded = day_cat_encoder.transform([[category_day_val]]).flatten()


    # Placeholder values for lat, long, merch_lat, merch_long (if not provided as input)
    # These were present in X_train, so we need placeholders. Using 0s as done before.
    lat, long, merch_lat, merch_long = 0, 0, 0, 0

    # Construct the full feature vector matching the *exact* order of X_train columns
    # Based on previous analysis:
    # ['amt', 'gender', 'lat', 'long', 'merch_lat', 'merch_long',
    # 'category_grocery_pos', 'category_misc_net', 'category_others', 'category_shopping_net', 'category_shopping_pos',
    # 'is_amt_greater_than_200',
    # 'time_category_night',
    # 'category_day'] # This last one corresponds to category_day_cat2

    # Get encoded column names to ensure correct ordering
    category_cols_ordered = category_encoder.get_feature_names_out(['category'])
    time_cat_cols_ordered = time_cat_encoder.get_feature_names_out(['time_category'])
    day_cat_cols_ordered = day_cat_encoder.get_feature_names_out(['category_day'])

    # Create dictionaries for easy lookup
    category_encoded_dict = dict(zip(category_cols_ordered, category_encoded))
    time_cat_encoded_dict = dict(zip(time_cat_cols_ordered, time_cat_encoded))
    day_cat_encoded_dict = dict(zip(day_cat_cols_ordered, cat_day_encoded))

    # Manually construct the input row based on the expected X_train column order
    input_row = [amt, gender_processed, lat, long, merch_lat, merch_long]

    # Add category encoded values in the order they appeared in X_train
    # Assumes the order is 'category_grocery_pos', 'category_misc_net', 'category_others', 'category_shopping_net', 'category_shopping_pos'
    input_row.append(category_encoded_dict.get('category_grocery_pos', 0.0))
    input_row.append(category_encoded_dict.get('category_misc_net', 0.0))
    input_row.append(category_encoded_dict.get('category_others', 0.0))
    input_row.append(category_encoded_dict.get('category_shopping_net', 0.0))
    input_row.append(category_encoded_dict.get('category_shopping_pos', 0.0))

    # Add is_amt_greater_than_200
    input_row.append(is_amt_greater_than_200)

    # Add time_category encoded value ('time_category_night')
    input_row.append(time_cat_encoded_dict.get('time_category_night', 0.0))

    # Add category_day encoded value ('category_day_cat2')
    input_row.append(day_cat_encoded_dict.get('category_day_cat2', 0.0))


    input_features = np.array(input_row).astype(np.float64) # Ensure float type for scaler

    # Feature scaling
    input_scaled = scaler.transform([input_features])
    input_tensor = torch.tensor(input_scaled, dtype=torch.float32).to(device)

    with torch.no_grad():
        output = torch.sigmoid(model(input_tensor))
        prediction = output.item()

    return {"Fraud Probability": round(prediction, 4), "Prediction": "Fraud" if prediction > 0.5 else "Not Fraud"}

# Gradio UI
category_options = ['grocery_pos', 'shopping_net', 'misc_net', 'shopping_pos', 'gas_transport', 'others']
interface = gr.Interface(
    fn=predict_fraud,
    inputs=[
        gr.Dropdown(category_options, label="Category"),
        gr.Radio(["M", "F"], label="Gender"),
        gr.Number(label="Transaction Amount (USD)"),
        gr.Slider(0, 23, step=1, label="Hour of Transaction"),
        gr.Slider(0, 6, step=1, label="Day of Week (0=Mon, 6=Sun)"),
        gr.Slider(1, 12, step=1, label="Month")
    ],
    outputs=[
        gr.Number(label="Fraud Probability"),
        gr.Text(label="Prediction")
    ],
    title="Fraud Detection Model",
    description="Enter transaction details to predict if it's fraudulent."
)

interface.launch()

df_train['gender']=df_train['gender'].apply(M_1_F_0)
df_test['gender']=df_test['gender'].apply(M_1_F_0)

df_train.street.value_counts()

df_train[df_train.is_fraud==1].city.value_counts()

df_train['zip'].value_counts()

df_train.drop(columns=['street','city','state','zip','dob','trans_num','city_pop'],inplace=True)
df_test.drop(columns=['street','city','state','zip','dob','trans_num','city_pop'],inplace=True)
df_train.head()

df_train.drop(columns=['unix_time'],inplace=True)
df_test.drop(columns=['unix_time'],inplace=True)

df_train.head()

df_train.job.value_counts()

df_train[df_train.is_fraud==1].job.value_counts()

(62+56+53+51+50)/(len(df_train[df_train.is_fraud==1]))*100

df_train.drop(columns=['job'],inplace=True)
df_test.drop(columns=['job'],inplace=True)
df_train.head()

ax=sns.histplot(x='amt',data=df_train[df_train.amt<=1000],hue='is_fraud',stat='percent',multiple='dodge',common_norm=False,bins=25)
ax.set_ylabel('Percentage in Each Type')
ax.set_xlabel('Transaction Amount in USD')
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

df_train[(df_train.amt>=200)& (df_train.is_fraud==1)].amt.sum()/df_train[df_train.is_fraud==1].amt.sum()*100
!pip install gradio

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df_train = pd.read_csv('fraudTrain.csv', index_col=0)
df_test = pd.read_csv('fraudTest.csv',index_col=0)

df_train.head()

df_train.columns

row,col=df_train.shape
print(f'The dataset has {row} rows and {col} columns.')

print(df_train.describe())

df_train.info()

fraud_percentage=len(df_train[df_train['is_fraud']==1])/len(df_train)*100
non_fraud_percentage=100-fraud_percentage
print(f'The percentage of fraud is {fraud_percentage} % and the percentage of non-fraud is {non_fraud_percentage} %')
print('This shows how imbalanced is the dataset')

df_train.isna().sum()

len(df_train)==len(df_train.drop_duplicates())

df_train.head()

df_train['cc_num'].value_counts()

df_train[df_train['is_fraud']==1].cc_num.value_counts()

df_train.drop(columns=['cc_num'],inplace=True)
df_test.drop(columns=['cc_num'],inplace=True)

df_train.merchant.value_counts()

df_train[df_train['is_fraud']==1].merchant.value_counts().head(50)

df_train.drop(columns=['merchant'],inplace=True)
df_test.drop(columns=['merchant'],inplace=True)

df_train.category.value_counts()

df_train[df_train['is_fraud']==1].category.value_counts()

(1743+1713+915+843+618)/(df_train[df_train['is_fraud']==1].category.value_counts().sum())*100

df_train.groupby('category')['amt'].sum().sort_values(ascending=False)

(14460822.38+9307993.61+8625149.68+8351732.29+7173928.11)/(df_train.groupby('category')['amt'].sum().sum())*100

def convert_to_other(instance):
    if(instance in ['grocery_pos', 'shopping_net', 'misc_net','shopping_pos', 'gas_transport']):
        return instance
    else :
        return 'others'
df_train['category']=df_train['category'].apply(convert_to_other)
df_test['category']=df_test['category'].apply(convert_to_other)
df_train.category.value_counts()

df_train.head()

from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(drop='first', sparse_output=False)
# drop='first' removes one column to avoid redundancy which also remove multicollinearity
# if we don't remove the input feature will have depencies. sum of values in the col will be 1 which can effect the performance

encoded_array = encoder.fit_transform(df_train[['category']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category']))

df_train = pd.concat([df_train.drop(columns=['category']), encoded_df], axis=1)

encoded_array = encoder.transform(df_test[['category']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category']))

df_test = pd.concat([df_test.drop(columns=['category']), encoded_df], axis=1)

df_train.drop(columns=['first','last'],inplace=True)
df_test.drop(columns=['first','last'],inplace=True)

def M_1_F_0(instance):
    if(instance=='M'):
        return 1
    return 0

df_train['gender']=df_train['gender'].apply(M_1_F_0)
df_test['gender']=df_test['gender'].apply(M_1_F_0)

df_train.street.value_counts()

df_train[df_train.is_fraud==1].city.value_counts()

df_train['zip'].value_counts()

df_train.drop(columns=['street','city','state','zip','dob','trans_num','city_pop'],inplace=True)
df_test.drop(columns=['street','city','state','zip','dob','trans_num','city_pop'],inplace=True)
df_train.head()

df_train.drop(columns=['unix_time'],inplace=True)
df_test.drop(columns=['unix_time'],inplace=True)

df_train.head()

df_train.job.value_counts()

df_train[df_train.is_fraud==1].job.value_counts()

(62+56+53+51+50)/(len(df_train[df_train.is_fraud==1]))*100

df_train.drop(columns=['job'],inplace=True)
df_test.drop(columns=['job'],inplace=True)
df_train.head()

ax=sns.histplot(x='amt',data=df_train[df_train.amt<=1000],hue='is_fraud',stat='percent',multiple='dodge',common_norm=False,bins=25)
ax.set_ylabel('Percentage in Each Type')
ax.set_xlabel('Transaction Amount in USD')
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

df_train[(df_train.amt>=200)& (df_train.is_fraud==1)].amt.sum()/df_train[df_train.is_fraud==1].amt.sum()*100

def is_greater_than_200(amt):
    if(amt>=200):
        return 1
    return 0
df_train['is_amt_greater_than_200']=df_train.amt.apply(is_greater_than_200)
df_test['is_amt_greater_than_200']=df_test.amt.apply(is_greater_than_200)

split_sum = df_train[df_train.is_fraud==1].groupby('is_amt_greater_than_200')['amt'].sum()

plt.bar(split_sum.index.astype(str), split_sum.values, color=['blue', 'orange'])

df_train['hour']=pd.to_datetime(df_train['trans_date_trans_time']).dt.hour
df_test['hour']=pd.to_datetime(df_test['trans_date_trans_time']).dt.hour
ax=sns.histplot(data=df_train, x="hour", hue="is_fraud", common_norm=False,stat='percent',multiple='dodge')
ax.set_ylabel('Percentage')
ax.set_xlabel('Time (Hour) in a Day')
plt.xticks(np.arange(0,24,1))
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

df_train[(df_train.is_fraud == 1) & ((df_train.hour <= 3) | (df_train.hour >= 22))]['amt'].sum()/(df_train[df_train.is_fraud==1].amt.sum())*100

def time_category(hour):
    if hour>=22 or hour<=3 :
        return "night"
    else :
        return "day"

df_train['time_category']= df_train['hour'].apply(time_category)
df_test['time_category']= df_test['hour'].apply(time_category)

df_train.time_category.value_counts()

df_train.info()

encoder = OneHotEncoder(drop='first', sparse_output=False)
encoded_array = encoder.fit_transform(df_train[['time_category']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['time_category']))

df_train = pd.concat([df_train.drop(columns=['time_category']), encoded_df], axis=1)

encoded_array = encoder.transform(df_test[['time_category']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['time_category']))

df_test = pd.concat([df_test.drop(columns=['time_category']), encoded_df], axis=1)

df_train.info()

df_train.head()

df_train['day']=pd.to_datetime(df_train['trans_date_trans_time']).dt.dayofweek
df_test['day']=pd.to_datetime(df_test['trans_date_trans_time']).dt.dayofweek
ax=sns.histplot(data=df_train, x="day", hue="is_fraud", common_norm=False,stat='percent',multiple='dodge')
#ax.set_xticklabels(['',"Mon","Tue","Wed","Thu","Fri","Sat","Sun"])
plt.xticks(np.arange(7), ["Mon","Tue","Wed","Thu","Fri","Sat","Sun"]) #set the xtick labels
ax.set_ylabel('Percentage')
ax.set_xlabel('Day of Week')
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

df_train[(df_train.is_fraud == 1) & ((df_train.day >= 5)|(df_train.day==0))]['amt'].sum() / df_train[df_train.is_fraud == 1]['amt'].sum()*100

def category_day(day):
    if(day==0 or day>=5):
        return "cat1"
    else:
        return "cat2"
df_train['category_day']=df_train['day'].apply(category_day)
df_test['category_day']=df_test['day'].apply(category_day)

encoded_array = encoder.fit_transform(df_train[['category_day']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category_day']))

df_train = pd.concat([df_train.drop(columns=['category_day']), encoded_df], axis=1)

encoded_array = encoder.transform(df_test[['category_day']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category_day']))

df_test = pd.concat([df_test.drop(columns=['category_day']), encoded_df], axis=1)

df_test['category_day']=df_test['category_day_cat2']
df_test.drop(columns=['category_day_cat2'],inplace=True)

df_train['category_day']=df_train['category_day_cat2']
df_train.drop(columns=['category_day_cat2'],inplace=True)

df_train.category_day.value_counts()

df_fraud = df_train[df_train['is_fraud'] == 1]  # Keep only fraud transactions

ax = sns.histplot(data=df_fraud, x="day", common_norm=False, stat='percent', multiple='dodge')

ax.set_xticklabels(["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"])
ax.set_ylabel('Percentage')
ax.set_xlabel('Day of Week')
plt.title('Fraud Transactions by Day of the Week')
plt.show()

df_train['month']=pd.to_datetime(df_train['trans_date_trans_time']).dt.month
df_test['month']=pd.to_datetime(df_test['trans_date_trans_time']).dt.month
ax=sns.histplot(data=df_train, x="month", hue="is_fraud", common_norm=False,stat='percent',multiple='dodge')
ax.set_ylabel('Percentage')
ax.set_xlabel('Month')
plt.xticks(np.arange(1,13,1))
ax.set_xticklabels(["Jan","Feb","Mar","Apr","May","Jun","Jul",'Aug','Sep','Oct','Nov','Dec'])
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

df_fraud = df_train[df_train['is_fraud'] == 1]  # Keep only fraud transactions

ax = sns.histplot(data=df_fraud, x="month", common_norm=False, stat='percent', multiple='dodge')

ax.set_ylabel('Percentage')
ax.set_xlabel('Month')

plt.xticks(np.arange(1, 13, 1), ["Jan", "Feb", "Mar", "Apr", "May", "Jun",
                                 "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"])

plt.title('Fraud Transactions by Month')
plt.show()

df_train.head()

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np

X_train = df_train.drop(columns=['trans_date_trans_time','day','hour','is_fraud','month']).copy()
y_train = df_train['is_fraud'].copy()

X_test = df_test.drop(columns=['trans_date_trans_time','day','hour','is_fraud','month']).copy()
y_test = df_test['is_fraud'].copy()

#Feature Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)

X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)

class FraudDataset(Dataset):
    def __init__(self, features, labels):
        self.features = features
        self.labels = labels
    def __len__(self):
        return len(self.features)
    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

train_dataset = FraudDataset(X_train_tensor, y_train_tensor)
test_dataset = FraudDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=512, pin_memory=True)

class BinaryClassifier(nn.Module):
    def __init__(self, input_dim):
        super(BinaryClassifier, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(32, 1)
        )
    def forward(self, x):
        return self.model(x)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BinaryClassifier(X_train_tensor.shape[1]).to(device)

criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
# Training Loop
epochs = 20
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for features, labels in train_loader:
        features, labels = features.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(features)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    avg_loss = total_loss / len(train_loader)
    print(f"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}")

#  Evaluation
model.eval()
correct = 0
total = 0
with torch.no_grad():
    for features, labels in test_loader:
        features, labels = features.to(device), labels.to(device)
        outputs = model(features)
        predicted = torch.sigmoid(outputs)
        predicted = (predicted > 0.5).float()
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
accuracy = correct / total
print(f"Test Accuracy: {accuracy:.4f}")

from sklearn.metrics import precision_score, recall_score, f1_score

model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for features, labels in test_loader:
        features = features.to(device)
        outputs = model(features)
        preds = torch.sigmoid(outputs).cpu().numpy()
        all_preds.extend(preds)
        all_labels.extend(labels.numpy())

# Convert predictions to binary labels
# Ensure all_preds is a flat list or array before converting to binary
all_preds_flat = [p for sublist in all_preds for p in (sublist if isinstance(sublist, (list, np.ndarray)) else [sublist])]
binary_preds = [1 if p > 0.5 else 0 for p in all_preds_flat]

# Ensure all_labels is a flat list or array
all_labels_flat = [l for sublist in all_labels for l in (sublist if isinstance(sublist, (list, np.ndarray)) else [sublist])]

# Calculate metrics
precision = precision_score(all_labels_flat, binary_preds)
recall = recall_score(all_labels_flat, binary_preds)
f1 = f1_score(all_labels_flat, binary_preds)

print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")
print(f"F1 Score:  {f1:.4f}")

amt_saved = df_test[(binary_preds==y_test) & (df_test.is_fraud==1)].amt.sum()

amt_saved

fraud_amt = df_test[df_test.is_fraud==1].amt.sum()

fraud_amt

saved_percentage=amt_saved/fraud_amt*100

print(f'Using this model saves {saved_percentage} % of fraud money')

#dly

import gradio as gr
import torch
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import pandas as pd
import joblib

# Load the model components
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Assuming your model and scaler are saved like this after training
# torch.save(model.state_dict(), "fraud_model.pth")
# joblib.dump(scaler, "scaler.pkl")
# joblib.dump(encoder, "encoder.pkl")

torch.save(model.state_dict(), "fraud_model.pth")
joblib.dump(scaler, "scaler.pkl")
# You should save the specific encoders used for category, time_category, day_category
# during training, not the 'encoder' variable which was reused.
# Let's re-fit them for the Gradio app as done in the corrected Gradio block below.
# joblib.dump(category_encoder, "category_encoder.pkl")
# joblib.dump(time_cat_encoder, "time_cat_encoder.pkl")
# joblib.dump(day_cat_encoder, "day_cat_encoder.pkl")


class BinaryClassifier(torch.nn.Module):
    def __init__(self, input_dim):
        super(BinaryClassifier, self).__init__()
        self.model = torch.nn.Sequential(
            torch.nn.Linear(input_dim, 128),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(128, 64),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(64, 32),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(32, 1)
        )
    def forward(self, x):
        return self.model(x)

# Load scaler
scaler = joblib.load("scaler.pkl")

# Re-fit the specific encoders used in the notebook for prediction
# You need access to the training data or a sample with all possible values
# to fit the encoders correctly. Using df_train here assuming it's available.
# If df_train isn't available in the Gradio environment, you'd need to save and load these encoders.
if 'df_train' in locals():
    category_encoder = OneHotEncoder(drop='first', sparse_output=False)
    category_encoder.fit(df_train[['category']].astype(str))

    time_cat_encoder = OneHotEncoder(drop='first', sparse_output=False)
    time_cat_encoder.fit(df_train[['time_category']].astype(str))

    day_cat_encoder = OneHotEncoder(drop='first', sparse_output=False)
    day_cat_encoder.fit(df_train[['category_day']].astype(str))
else:
    # If df_train is not available, load pre-fitted encoders
    # category_encoder = joblib.load("category_encoder.pkl")
    # time_cat_encoder = joblib.load("time_cat_encoder.pkl")
    # day_cat_encoder = joblib.load("day_cat_encoder.pkl")
    # Placeholder/Error handling if encoders aren't available
    raise FileNotFoundError("Training data (df_train) not found. Please ensure it's available or load pre-fitted encoders.")


# Determine the input dimension based on the final X_train columns structure
# This relies on knowing the structure of X_train after preprocessing.
# If you saved X_train columns, you could load them. Otherwise, manually construct based on logic.
# Based on the manual reconstruction in the previous block, the input dimension is 14.
input_dim = 14 # This should match X_train_tensor.shape[1] from training

model = BinaryClassifier(input_dim)
model.load_state_dict(torch.load("fraud_model.pth", map_location=device))
model.to(device)
model.eval()

# Define prediction function
def predict_fraud(category, gender, amt, hour, day, month):
    # Preprocessing replicating training steps
    gender_processed = 1 if gender == "M" else 0
    is_amt_greater_than_200 = 1 if amt >= 200 else 0
    time_category_val = "night" if (hour >= 22 or hour <= 3) else "day"
    category_day_val = "cat1" if (day == 0 or day >= 5) else "cat2"

    # One-hot encode using the specific encoders
    category_encoded = category_encoder.transform([[category]]).flatten()
    time_cat_encoded = time_cat_encoder.transform([[time_category_val]]).flatten()
    cat_day_encoded = day_cat_encoder.transform([[category_day_val]]).flatten()


    # Placeholder values for lat, long, merch_lat, merch_long (if not provided as input)
    # These were present in X_train, so we need placeholders. Using 0s as done before.
    lat, long, merch_lat, merch_long = 0, 0, 0, 0

    # Construct the full feature vector matching the *exact* order of X_train columns
    # Based on previous analysis:
    # ['amt', 'gender', 'lat', 'long', 'merch_lat', 'merch_long',
    # 'category_grocery_pos', 'category_misc_net', 'category_others', 'category_shopping_net', 'category_shopping_pos',
    # 'is_amt_greater_than_200',
    # 'time_category_night',
    # 'category_day'] # This last one corresponds to category_day_cat2

    # Get encoded column names to ensure correct ordering
    category_cols_ordered = category_encoder.get_feature_names_out(['category'])
    time_cat_cols_ordered = time_cat_encoder.get_feature_names_out(['time_category'])
    day_cat_cols_ordered = day_cat_encoder.get_feature_names_out(['category_day'])

    # Create dictionaries for easy lookup
    category_encoded_dict = dict(zip(category_cols_ordered, category_encoded))
    time_cat_encoded_dict = dict(zip(time_cat_cols_ordered, time_cat_encoded))
    day_cat_encoded_dict = dict(zip(day_cat_cols_ordered, cat_day_encoded))

    # Manually construct the input row based on the expected X_train column order
    input_row = [amt, gender_processed, lat, long, merch_lat, merch_long]

    # Add category encoded values in the order they appeared in X_train
    # Assumes the order is 'category_grocery_pos', 'category_misc_net', 'category_others', 'category_shopping_net', 'category_shopping_pos'
    input_row.append(category_encoded_dict.get('category_grocery_pos', 0.0))
    input_row.append(category_encoded_dict.get('category_misc_net', 0.0))
    input_row.append(category_encoded_dict.get('category_others', 0.0))
    input_row.append(category_encoded_dict.get('category_shopping_net', 0.0))
    input_row.append(category_encoded_dict.get('category_shopping_pos', 0.0))

    # Add is_amt_greater_than_200
    input_row.append(is_amt_greater_than_200)

    # Add time_category encoded value ('time_category_night')
    input_row.append(time_cat_encoded_dict.get('time_category_night', 0.0))

    # Add category_day encoded value ('category_day_cat2')
    input_row.append(day_cat_encoded_dict.get('category_day_cat2', 0.0))


    input_features = np.array(input_row).astype(np.float64) # Ensure float type for scaler

    # Feature scaling
    input_scaled = scaler.transform([input_features])
    input_tensor = torch.tensor(input_scaled, dtype=torch.float32).to(device)

    with torch.no_grad():
        output = torch.sigmoid(model(input_tensor))
        prediction = output.item()

    return {"Fraud Probability": round(prediction, 4), "Prediction": "Fraud" if prediction > 0.5 else "Not Fraud"}

# Gradio UI
category_options = ['grocery_pos', 'shopping_net', 'misc_net', 'shopping_pos', 'gas_transport', 'others']
interface = gr.Interface(
    fn=predict_fraud,
    inputs=[
        gr.Dropdown(category_options, label="Category"),
        gr.Radio(["M", "F"], label="Gender"),
        gr.Number(label="Transaction Amount (USD)"),
        gr.Slider(0, 23, step=1, label="Hour of Transaction"),
        gr.Slider(0, 6, step=1, label="Day of Week (0=Mon, 6=Sun)"),
        gr.Slider(1, 12, step=1, label="Month")
    ],
    outputs=[
        gr.Number(label="Fraud Probability"),
        gr.Text(label="Prediction")
    ],
    title="Fraud Detection Model",
    description="Enter transaction details to predict if it's fraudulent."
)

interface.launch()
df_train['is_amt_greater_than_200']=df_train.amt.apply(is_greater_than_200)
df_test['is_amt_greater_than_200']=df_test.amt.apply(is_greater_than_200)

split_sum = df_train[df_train.is_fraud==1].groupby('is_amt_greater_than_200')['amt'].sum()

plt.bar(split_sum.index.astype(str), split_sum.values, color=['blue', 'orange'])

df_train['hour']=pd.to_datetime(df_train['trans_date_trans_time']).dt.hour
df_test['hour']=pd.to_datetime(df_test['trans_date_trans_time']).dt.hour
ax=sns.histplot(data=df_train, x="hour", hue="is_fraud", common_norm=False,stat='percent',multiple='dodge')
ax.set_ylabel('Percentage')
ax.set_xlabel('Time (Hour) in a Day')
plt.xticks(np.arange(0,24,1))
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

def time_category(hour):
    if hour>=22 or hour<=3 :
        return "night"
    else :
        return "day"

df_train['time_category']= df_train['hour'].apply(time_category)
df_test['time_category']= df_test['hour'].apply(time_category)

df_train.time_category.value_counts()

df_train.info()

encoder = OneHotEncoder(drop='first', sparse_output=False)
encoded_array = encoder.fit_transform(df_train[['time_category']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['time_category']))

df_train = pd.concat([df_train.drop(columns=['time_category']), encoded_df], axis=1)

encoded_array = encoder.transform(df_test[['time_category']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['time_category']))

df_test = pd.concat([df_test.drop(columns=['time_category']), encoded_df], axis=1)

df_train.info()

df_train.head()

df_train['day']=pd.to_datetime(df_train['trans_date_trans_time']).dt.dayofweek
df_test['day']=pd.to_datetime(df_test['trans_date_trans_time']).dt.dayofweek
ax=sns.histplot(data=df_train, x="day", hue="is_fraud", common_norm=False,stat='percent',multiple='dodge')
#ax.set_xticklabels(['',"Mon","Tue","Wed","Thu","Fri","Sat","Sun"])
plt.xticks(np.arange(7), ["Mon","Tue","Wed","Thu","Fri","Sat","Sun"]) #set the xtick labels
ax.set_ylabel('Percentage')
ax.set_xlabel('Day of Week')
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

df_train[(df_train.is_fraud == 1) & ((df_train.day >= 5)|(df_train.day==0))]['amt'].sum() / df_train[df_train.is_fraud == 1]['amt'].sum()*100

def category_day(day):
    if(day==0 or day>=5):
        return "cat1"
    else:
        return "cat2"
df_train['category_day']=df_train['day'].apply(category_day)
df_test['category_day']=df_test['day'].apply(category_day)

encoded_array = encoder.fit_transform(df_train[['category_day']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category_day']))

df_train = pd.concat([df_train.drop(columns=['category_day']), encoded_df], axis=1)

encoded_array = encoder.transform(df_test[['category_day']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category_day']))

df_test = pd.concat([df_test.drop(columns=['category_day']), encoded_df], axis=1)

df_test['category_day']=df_test['category_day_cat2']
df_test.drop(columns=['category_day_cat2'],inplace=True)

df_train['category_day']=df_train['category_day_cat2']
df_train.drop(columns=['category_day_cat2'],inplace=True)

df_train.category_day.value_counts()

df_fraud = df_train[df_train['is_fraud'] == 1]  # Keep only fraud transactions

ax = sns.histplot(data=df_fraud, x="day", common_norm=False, stat='percent', multiple='dodge')

ax.set_xticklabels(["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"])
ax.set_ylabel('Percentage')
ax.set_xlabel('Day of Week')
plt.title('Fraud Transactions by Day of the Week')
plt.show()

df_train['month']=pd.to_datetime(df_train['trans_date_trans_time']).dt.month
df_test['month']=pd.to_datetime(df_test['trans_date_trans_time']).dt.month
ax=sns.histplot(data=df_train, x="month", hue="is_fraud", common_norm=False,stat='percent',multiple='dodge')
ax.set_ylabel('Percentage')
ax.set_xlabel('Month')
plt.xticks(np.arange(1,13,1))
ax.set_xticklabels(["Jan","Feb","Mar","Apr","May","Jun","Jul",'Aug','Sep','Oct','Nov','Dec'])
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

df_fraud = df_train[df_train['is_fraud'] == 1]  # Keep only fraud transactions

ax = sns.histplot(data=df_fraud, x="month", common_norm=False, stat='percent', multiple='dodge')

ax.set_ylabel('Percentage')
ax.set_xlabel('Month')

plt.xticks(np.arange(1, 13, 1), ["Jan", "Feb", "Mar", "Apr", "May", "Jun",
                                 "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"])

plt.title('Fraud Transactions by Month')
plt.show()

df_train.head()

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np

X_train = df_train.drop(columns=['trans_date_trans_time','day','hour','is_fraud','month']).copy()
y_train = df_train['is_fraud'].copy()

X_test = df_test.drop(columns=['trans_date_trans_time','day','hour','is_fraud','month']).copy()
y_test = df_test['is_fraud'].copy()

#Feature Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)

X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)

class FraudDataset(Dataset):
    def __init__(self, features, labels):
        self.features = features
        self.labels = labels
    def __len__(self):
        return len(self.features)
    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

train_dataset = FraudDataset(X_train_tensor, y_train_tensor)
test_dataset = FraudDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=512, pin_memory=True)

class BinaryClassifier(nn.Module):
    def __init__(self, input_dim):
        super(BinaryClassifier, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(32, 1)
        )
    def forward(self, x):
        return self.model(x)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BinaryClassifier(X_train_tensor.shape[1]).to(device)

criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
# Training Loop
epochs = 20
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for features, labels in train_loader:
        features, labels = features.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(features)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    avg_loss = total_loss / len(train_loader)
    print(f"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}")

#  Evaluation
model.eval()
correct = 0
total = 0
with torch.no_grad():
    for features, labels in test_loader:
        features, labels = features.to(device), labels.to(device)
        outputs = model(features)
        predicted = torch.sigmoid(outputs)
        predicted = (predicted > 0.5).float()
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
accuracy = correct / total
print(f"Test Accuracy: {accuracy:.4f}")

from sklearn.metrics import precision_score, recall_score, f1_score

model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for features, labels in test_loader:
        features = features.to(device)
        outputs = model(features)
        preds = torch.sigmoid(outputs).cpu().numpy()
        all_preds.extend(preds)
        all_labels.extend(labels.numpy())

# Convert predictions to binary labels
# Ensure all_preds is a flat list or array before converting to binary
all_preds_flat = [p for sublist in all_preds for p in (sublist if isinstance(sublist, (list, np.ndarray)) else [sublist])]
binary_preds = [1 if p > 0.5 else 0 for p in all_preds_flat]

# Ensure all_labels is a flat list or array
all_labels_flat = [l for sublist in all_labels for l in (sublist if isinstance(sublist, (list, np.ndarray)) else [sublist])]

# Calculate metrics
precision = precision_score(all_labels_flat, binary_preds)
recall = recall_score(all_labels_flat, binary_preds)
f1 = f1_score(all_labels_flat, binary_preds)

print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")
print(f"F1 Score:  {f1:.4f}")

amt_saved = df_test[(binary_preds==y_test) & (df_test.is_fraud==1)].amt.sum()

amt_saved

fraud_amt = df_test[df_test.is_fraud==1].amt.sum()

fraud_amt

saved_percentage=amt_saved/fraud_amt*100

print(f'Using this model saves {saved_percentage} % of fraud money')

#dly

import gradio as gr
import torch
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import pandas as pd
import joblib

# Load the model components
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Assuming your model and scaler are saved like this after training
# torch.save(model.state_dict(), "fraud_model.pth")
# joblib.dump(scaler, "scaler.pkl")
# joblib.dump(encoder, "encoder.pkl")

torch.save(model.state_dict(), "fraud_model.pth")
joblib.dump(scaler, "scaler.pkl")
joblib.dump(encoder, "encoder.pkl")

class BinaryClassifier(torch.nn.Module):
    def __init__(self, input_dim):
        super(BinaryClassifier, self).__init__()
        self.model = torch.nn.Sequential(
            torch.nn.Linear(input_dim, 128),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(128, 64),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(64, 32),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(32, 1)
        )
    def forward(self, x):
        return self.model(x)

# Load scaler and encoders
scaler = joblib.load("scaler.pkl")
encoder = joblib.load("encoder.pkl")

# Instantiate and load model
input_dim = scaler.transform(np.zeros((1, scaler.mean_.shape[0]))).shape[1]
model = BinaryClassifier(input_dim)
model.load_state_dict(torch.load("fraud_model.pth", map_location=device))
model.to(device)
model.eval()

# Define prediction function
def predict_fraud(category, gender, amt, hour, day, month):
    # Preprocessing
    gender = 1 if gender == "M" else 0
    is_amt_greater_than_200 = 1 if amt >= 200 else 0
    time_category = "night" if (hour >= 22 or hour <= 3) else "day"
    category_day = "cat1" if (day == 0 or day >= 5) else "cat2"

    # One-hot encode category features
    category_encoded = encoder.transform([[category]]).flatten()
    time_cat_encoded = encoder.transform([[time_category]]).flatten()
    cat_day_encoded = encoder.transform([[category_day]]).flatten()

    # Prepare full feature vector
    input_features = np.concatenate([
        [amt, gender, is_amt_greater_than_200, hour, day, month],
        category_encoded,
        time_cat_encoded,
        cat_day_encoded
    ])

    # Feature scaling
    input_scaled = scaler.transform([input_features])
    input_tensor = torch.tensor(input_scaled, dtype=torch.float32).to(device)

    with torch.no_grad():
        output = torch.sigmoid(model(input_tensor))
        prediction = output.item()

    return {"Fraud Probability": round(prediction, 4), "Prediction": "Fraud" if prediction > 0.5 else "Not Fraud"}

# Gradio UI
category_options = ['grocery_pos', 'shopping_net', 'misc_net', 'shopping_pos', 'gas_transport', 'others']
interface = gr.Interface(
    fn=predict_fraud,
    inputs=[
        gr.Dropdown(category_options, label="Category"),
        gr.Radio(["M", "F"], label="Gender"),
        gr.Number(label="Transaction Amount (USD)"),
        gr.Slider(0, 23, step=1, label="Hour of Transaction"),
        gr.Slider(0, 6, step=1, label="Day of Week (0=Mon, 6=Sun)"),
        gr.Slider(1, 12, step=1, label="Month")
    ],
    outputs=[
        gr.Number(label="Fraud Probability"),
        gr.Text(label="Prediction")
    ],
    title="Fraud Detection Model",
    description="Enter transaction details to predict if it's fraudulent."
)
def time_category(hour):
    if hour>=22 or hour<=3 :
        return "night"
    else :
        return "day"

df_train['time_category']= df_train['hour'].apply(time_category)
df_test['time_category']= df_test['hour'].apply(time_category)

df_train.time_category.value_counts()

df_train.info()

encoder = OneHotEncoder(drop='first', sparse_output=False)
encoded_array = encoder.fit_transform(df_train[['time_category']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['time_category']))

df_train = pd.concat([df_train.drop(columns=['time_category']), encoded_df], axis=1)

encoded_array = encoder.transform(df_test[['time_category']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['time_category']))

df_test = pd.concat([df_test.drop(columns=['time_category']), encoded_df], axis=1)

df_train.info()

df_train.head()

df_train['day']=pd.to_datetime(df_train['trans_date_trans_time']).dt.dayofweek
df_test['day']=pd.to_datetime(df_test['trans_date_trans_time']).dt.dayofweek
ax=sns.histplot(data=df_train, x="day", hue="is_fraud", common_norm=False,stat='percent',multiple='dodge')
#ax.set_xticklabels(['',"Mon","Tue","Wed","Thu","Fri","Sat","Sun"])
plt.xticks(np.arange(7), ["Mon","Tue","Wed","Thu","Fri","Sat","Sun"]) #set the xtick labels
ax.set_ylabel('Percentage')
ax.set_xlabel('Day of Week')
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

df_train[(df_train.is_fraud == 1) & ((df_train.day >= 5)|(df_train.day==0))]['amt'].sum() / df_train[df_train.is_fraud == 1]['amt'].sum()*100

def category_day(day):
    if(day==0 or day>=5):
        return "cat1"
    else:
        return "cat2"
df_train['category_day']=df_train['day'].apply(category_day)
df_test['category_day']=df_test['day'].apply(category_day)

encoded_array = encoder.fit_transform(df_train[['category_day']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category_day']))

df_train = pd.concat([df_train.drop(columns=['category_day']), encoded_df], axis=1)

encoded_array = encoder.transform(df_test[['category_day']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category_day']))

df_test = pd.concat([df_test.drop(columns=['category_day']), encoded_df], axis=1)

df_test['category_day']=df_test['category_day_cat2']
df_test.drop(columns=['category_day_cat2'],inplace=True)

df_train['category_day']=df_train['category_day_cat2']
df_train.drop(columns=['category_day_cat2'],inplace=True)

df_train.category_day.value_counts()

df_fraud = df_train[df_train['is_fraud'] == 1]  # Keep only fraud transactions

ax = sns.histplot(data=df_fraud, x="day", common_norm=False, stat='percent', multiple='dodge')

ax.set_xticklabels(["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"])
ax.set_ylabel('Percentage')
ax.set_xlabel('Day of Week')
plt.title('Fraud Transactions by Day of the Week')
plt.show()

df_train['month']=pd.to_datetime(df_train['trans_date_trans_time']).dt.month
df_test['month']=pd.to_datetime(df_test['trans_date_trans_time']).dt.month
ax=sns.histplot(data=df_train, x="month", hue="is_fraud", common_norm=False,stat='percent',multiple='dodge')
ax.set_ylabel('Percentage')
ax.set_xlabel('Month')
plt.xticks(np.arange(1,13,1))
ax.set_xticklabels(["Jan","Feb","Mar","Apr","May","Jun","Jul",'Aug','Sep','Oct','Nov','Dec'])
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

df_fraud = df_train[df_train['is_fraud'] == 1]  # Keep only fraud transactions

ax = sns.histplot(data=df_fraud, x="month", common_norm=False, stat='percent', multiple='dodge')

ax.set_ylabel('Percentage')
ax.set_xlabel('Month')

plt.xticks(np.arange(1, 13, 1), ["Jan", "Feb", "Mar", "Apr", "May", "Jun",
                                 "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"])

plt.title('Fraud Transactions by Month')
plt.show()

df_train.head()

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np

X_train = df_train.drop(columns=['trans_date_trans_time','day','hour','is_fraud','month']).copy()
y_train = df_train['is_fraud'].copy()

X_test = df_test.drop(columns=['trans_date_trans_time','day','hour','is_fraud','month']).copy()
y_test = df_test['is_fraud'].copy()

#Feature Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)

X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)

class FraudDataset(Dataset):
    def __init__(self, features, labels):
        self.features = features
        self.labels = labels
    def __len__(self):
        return len(self.features)
    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

train_dataset = FraudDataset(X_train_tensor, y_train_tensor)
test_dataset = FraudDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=512, pin_memory=True)

class BinaryClassifier(nn.Module):
    def __init__(self, input_dim):
        super(BinaryClassifier, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(32, 1)
        )
    def forward(self, x):
        return self.model(x)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BinaryClassifier(X_train_tensor.shape[1]).to(device)

criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
# Training Loop
epochs = 20
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for features, labels in train_loader:
        features, labels = features.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(features)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    avg_loss = total_loss / len(train_loader)
    print(f"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}")

#  Evaluation
model.eval()
correct = 0
total = 0
with torch.no_grad():
    for features, labels in test_loader:
        features, labels = features.to(device), labels.to(device)
        outputs = model(features)
        predicted = torch.sigmoid(outputs)
        predicted = (predicted > 0.5).float()
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
accuracy = correct / total
print(f"Test Accuracy: {accuracy:.4f}")

from sklearn.metrics import precision_score, recall_score, f1_score

model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for features, labels in test_loader:
        features = features.to(device)
        outputs = model(features)
        preds = torch.sigmoid(outputs).cpu().numpy()
        all_preds.extend(preds)
        all_labels.extend(labels.numpy())

# Convert predictions to binary labels
# Ensure all_preds is a flat list or array before converting to binary
all_preds_flat = [p for sublist in all_preds for p in (sublist if isinstance(sublist, (list, np.ndarray)) else [sublist])]
binary_preds = [1 if p > 0.5 else 0 for p in all_preds_flat]

# Ensure all_labels is a flat list or array
all_labels_flat = [l for sublist in all_labels for l in (sublist if isinstance(sublist, (list, np.ndarray)) else [sublist])]

# Calculate metrics
precision = precision_score(all_labels_flat, binary_preds)
recall = recall_score(all_labels_flat, binary_preds)
f1 = f1_score(all_labels_flat, binary_preds)

print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")
print(f"F1 Score:  {f1:.4f}")

amt_saved = df_test[(binary_preds==y_test) & (df_test.is_fraud==1)].amt.sum()

amt_saved

fraud_amt = df_test[df_test.is_fraud==1].amt.sum()

fraud_amt

saved_percentage=amt_saved/fraud_amt*100

print(f'Using this model saves {saved_percentage} % of fraud money')

#dly

import gradio as gr
import torch
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import pandas as pd
import joblib

# Load the model components
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Assuming your model and scaler are saved like this after training
# torch.save(model.state_dict(), "fraud_model.pth")
# joblib.dump(scaler, "scaler.pkl")
# joblib.dump(encoder, "encoder.pkl")

torch.save(model.state_dict(), "fraud_model.pth")
joblib.dump(scaler, "scaler.pkl")
joblib.dump(encoder, "encoder.pkl")

class BinaryClassifier(torch.nn.Module):
    def __init__(self, input_dim):
        super(BinaryClassifier, self).__init__()
        self.model = torch.nn.Sequential(
            torch.nn.Linear(input_dim, 128),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(128, 64),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(64, 32),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(32, 1)
        )
    def forward(self, x):
        return self.model(x)

# Load scaler and encoders
scaler = joblib.load("scaler.pkl")
encoder = joblib.load("encoder.pkl")

# Instantiate and load model
input_dim = scaler.transform(np.zeros((1, scaler.mean_.shape[0]))).shape[1]
model = BinaryClassifier(input_dim)
model.load_state_dict(torch.load("fraud_model.pth", map_location=device))
model.to(device)
model.eval()

# Define prediction function
def predict_fraud(category, gender, amt, hour, day, month):
    # Preprocessing
    gender = 1 if gender == "M" else 0
    is_amt_greater_than_200 = 1 if amt >= 200 else 0
    time_category = "night" if (hour >= 22 or hour <= 3) else "day"
    category_day = "cat1" if (day == 0 or day >= 5) else "cat2"

    # One-hot encode category features
    category_encoded = encoder.transform([[category]]).flatten()
    time_cat_encoded = encoder.transform([[time_category]]).flatten()
    cat_day_encoded = encoder.transform([[category_day]]).flatten()

    # Prepare full feature vector
    input_features = np.concatenate([
        [amt, gender, is_amt_greater_than_200, hour, day, month],
        category_encoded,
        time_cat_encoded,
        cat_day_encoded
    ])

    # Feature scaling
    input_scaled = scaler.transform([input_features])
    input_tensor = torch.tensor(input_scaled, dtype=torch.float32).to(device)

    with torch.no_grad():
        output = torch.sigmoid(model(input_tensor))
        prediction = output.item()

    return {"Fraud Probability": round(prediction, 4), "Prediction": "Fraud" if prediction > 0.5 else "Not Fraud"}

# Gradio UI
category_options = ['grocery_pos', 'shopping_net', 'misc_net', 'shopping_pos', 'gas_transport', 'others']
interface = gr.Interface(
    fn=predict_fraud,
    inputs=[
        gr.Dropdown(category_options, label="Category"),
        gr.Radio(["M", "F"], label="Gender"),
        gr.Number(label="Transaction Amount (USD)"),
        gr.Slider(0, 23, step=1, label="Hour of Transaction"),
        gr.Slider(0, 6, step=1, label="Day of Week (0=Mon, 6=Sun)"),
        gr.Slider(1, 12, step=1, label="Month")
    ],
    outputs=[
        gr.Number(label="Fraud Probability"),
        gr.Text(label="Prediction")
    ],
    title="Fraud Detection Model",
    description="Enter transaction details to predict if it's fraudulent."
)

interface.launch()
df_train['time_category']= df_train['hour'].apply(time_category)
df_test['time_category']= df_test['hour'].apply(time_category)

df_train.time_category.value_counts()

df_train.info()

encoder = OneHotEncoder(drop='first', sparse_output=False)
encoded_array = encoder.fit_transform(df_train[['time_category']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['time_category']))

df_train = pd.concat([df_train.drop(columns=['time_category']), encoded_df], axis=1)

encoded_array = encoder.transform(df_test[['time_category']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['time_category']))

df_test = pd.concat([df_test.drop(columns=['time_category']), encoded_df], axis=1)

df_train.info()

df_train.head()

df_train['day']=pd.to_datetime(df_train['trans_date_trans_time']).dt.dayofweek
df_test['day']=pd.to_datetime(df_test['trans_date_trans_time']).dt.dayofweek
ax=sns.histplot(data=df_train, x="day", hue="is_fraud", common_norm=False,stat='percent',multiple='dodge')
#ax.set_xticklabels(['',"Mon","Tue","Wed","Thu","Fri","Sat","Sun"])
plt.xticks(np.arange(7), ["Mon","Tue","Wed","Thu","Fri","Sat","Sun"]) #set the xtick labels
ax.set_ylabel('Percentage')
ax.set_xlabel('Day of Week')
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

df_train[(df_train.is_fraud == 1) & ((df_train.day >= 5)|(df_train.day==0))]['amt'].sum() / df_train[df_train.is_fraud == 1]['amt'].sum()*100
def category_day(day):
    if(day==0 or day>=5):
        return "cat1"
    else:
        return "cat2"
df_train['category_day']=df_train['day'].apply(category_day)
df_test['category_day']=df_test['day'].apply(category_day)

encoded_array = encoder.fit_transform(df_train[['category_day']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category_day']))

df_train = pd.concat([df_train.drop(columns=['category_day']), encoded_df], axis=1)

encoded_array = encoder.transform(df_test[['category_day']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category_day']))

df_test = pd.concat([df_test.drop(columns=['category_day']), encoded_df], axis=1)

df_test['category_day']=df_test['category_day_cat2']
df_test.drop(columns=['category_day_cat2'],inplace=True)

df_train['category_day']=df_train['category_day_cat2']
df_train.drop(columns=['category_day_cat2'],inplace=True)

df_train.category_day.value_counts()

df_fraud = df_train[df_train['is_fraud'] == 1]  # Keep only fraud transactions

ax = sns.histplot(data=df_fraud, x="day", common_norm=False, stat='percent', multiple='dodge')

ax.set_xticklabels(["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"])
ax.set_ylabel('Percentage')
ax.set_xlabel('Day of Week')
plt.title('Fraud Transactions by Day of the Week')
plt.show()

df_train['month']=pd.to_datetime(df_train['trans_date_trans_time']).dt.month
df_test['month']=pd.to_datetime(df_test['trans_date_trans_time']).dt.month
ax=sns.histplot(data=df_train, x="month", hue="is_fraud", common_norm=False,stat='percent',multiple='dodge')
ax.set_ylabel('Percentage')
ax.set_xlabel('Month')
plt.xticks(np.arange(1,13,1))
ax.set_xticklabels(["Jan","Feb","Mar","Apr","May","Jun","Jul",'Aug','Sep','Oct','Nov','Dec'])
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

df_fraud = df_train[df_train['is_fraud'] == 1]  # Keep only fraud transactions

ax = sns.histplot(data=df_fraud, x="month", common_norm=False, stat='percent', multiple='dodge')

ax.set_ylabel('Percentage')
ax.set_xlabel('Month')

plt.xticks(np.arange(1, 13, 1), ["Jan", "Feb", "Mar", "Apr", "May", "Jun",
                                 "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"])

plt.title('Fraud Transactions by Month')
plt.show()

df_train.head()

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np

X_train = df_train.drop(columns=['trans_date_trans_time','day','hour','is_fraud','month']).copy()
y_train = df_train['is_fraud'].copy()

X_test = df_test.drop(columns=['trans_date_trans_time','day','hour','is_fraud','month']).copy()
y_test = df_test['is_fraud'].copy()

#Feature Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)

X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)

class FraudDataset(Dataset):
    def __init__(self, features, labels):
        self.features = features
        self.labels = labels
    def __len__(self):
        return len(self.features)
    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

train_dataset = FraudDataset(X_train_tensor, y_train_tensor)
test_dataset = FraudDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=512, pin_memory=True)

class BinaryClassifier(nn.Module):
    def __init__(self, input_dim):
        super(BinaryClassifier, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(32, 1)
        )
    def forward(self, x):
        return self.model(x)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BinaryClassifier(X_train_tensor.shape[1]).to(device)

criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
# Training Loop
epochs = 20
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for features, labels in train_loader:
        features, labels = features.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(features)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    avg_loss = total_loss / len(train_loader)
    print(f"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}")

#  Evaluation
model.eval()
correct = 0
total = 0
with torch.no_grad():
    for features, labels in test_loader:
        features, labels = features.to(device), labels.to(device)
        outputs = model(features)
        predicted = torch.sigmoid(outputs)
        predicted = (predicted > 0.5).float()
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
accuracy = correct / total
print(f"Test Accuracy: {accuracy:.4f}")

from sklearn.metrics import precision_score, recall_score, f1_score

model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for features, labels in test_loader:
        features = features.to(device)
        outputs = model(features)
        preds = torch.sigmoid(outputs).cpu().numpy()
        all_preds.extend(preds)
        all_labels.extend(labels.numpy())

# Convert predictions to binary labels
# Ensure all_preds is a flat list or array before converting to binary
all_preds_flat = [p for sublist in all_preds for p in (sublist if isinstance(sublist, (list, np.ndarray)) else [sublist])]
binary_preds = [1 if p > 0.5 else 0 for p in all_preds_flat]

# Ensure all_labels is a flat list or array
all_labels_flat = [l for sublist in all_labels for l in (sublist if isinstance(sublist, (list, np.ndarray)) else [sublist])]

# Calculate metrics
precision = precision_score(all_labels_flat, binary_preds)
recall = recall_score(all_labels_flat, binary_preds)
f1 = f1_score(all_labels_flat, binary_preds)

print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")
print(f"F1 Score:  {f1:.4f}")

amt_saved = df_test[(binary_preds==y_test) & (df_test.is_fraud==1)].amt.sum()

amt_saved

fraud_amt = df_test[df_test.is_fraud==1].amt.sum()

fraud_amt

saved_percentage=amt_saved/fraud_amt*100

print(f'Using this model saves {saved_percentage} % of fraud money')

#dly

import gradio as gr
import torch
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import pandas as pd
import joblib

# Load the model components
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Assuming your model and scaler are saved like this after training
# torch.save(model.state_dict(), "fraud_model.pth")
# joblib.dump(scaler, "scaler.pkl")
# joblib.dump(encoder, "encoder.pkl")

torch.save(model.state_dict(), "fraud_model.pth")
joblib.dump(scaler, "scaler.pkl")
joblib.dump(encoder, "encoder.pkl")

class BinaryClassifier(torch.nn.Module):
    def __init__(self, input_dim):
        super(BinaryClassifier, self).__init__()
        self.model = torch.nn.Sequential(
            torch.nn.Linear(input_dim, 128),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(128, 64),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(64, 32),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(32, 1)
        )
    def forward(self, x):
        return self.model(x)

# Load scaler and encoders
scaler = joblib.load("scaler.pkl")
encoder = joblib.load("encoder.pkl")

# Instantiate and load model
input_dim = scaler.transform(np.zeros((1, scaler.mean_.shape[0]))).shape[1]
model = BinaryClassifier(input_dim)
model.load_state_dict(torch.load("fraud_model.pth", map_location=device))
model.to(device)
model.eval()

# Define prediction function
def predict_fraud(category, gender, amt, hour, day, month):
    # Preprocessing
    gender = 1 if gender == "M" else 0
    is_amt_greater_than_200 = 1 if amt >= 200 else 0
    time_category = "night" if (hour >= 22 or hour <= 3) else "day"
    category_day = "cat1" if (day == 0 or day >= 5) else "cat2"

    # One-hot encode category features
    category_encoded = encoder.transform([[category]]).flatten()
    time_cat_encoded = encoder.transform([[time_category]]).flatten()
    cat_day_encoded = encoder.transform([[category_day]]).flatten()

    # Prepare full feature vector
    input_features = np.concatenate([
        [amt, gender, is_amt_greater_than_200, hour, day, month],
        category_encoded,
        time_cat_encoded,
        cat_day_encoded
    ])

    # Feature scaling
    input_scaled = scaler.transform([input_features])
    input_tensor = torch.tensor(input_scaled, dtype=torch.float32).to(device)

    with torch.no_grad():
        output = torch.sigmoid(model(input_tensor))
        prediction = output.item()

    return {"Fraud Probability": round(prediction, 4), "Prediction": "Fraud" if prediction > 0.5 else "Not Fraud"}

# Gradio UI
category_options = ['grocery_pos', 'shopping_net', 'misc_net', 'shopping_pos', 'gas_transport', 'others']
interface = gr.Interface(
    fn=predict_fraud,
    inputs=[
        gr.Dropdown(category_options, label="Category"),
        gr.Radio(["M", "F"], label="Gender"),
        gr.Number(label="Transaction Amount (USD)"),
        gr.Slider(0, 23, step=1, label="Hour of Transaction"),
        gr.Slider(0, 6, step=1, label="Day of Week (0=Mon, 6=Sun)"),
        gr.Slider(1, 12, step=1, label="Month")
    ],
    outputs=[
        gr.Number(label="Fraud Probability"),
        gr.Text(label="Prediction")
    ],
    title="Fraud Detection Model",
    description="Enter transaction details to predict if it's fraudulent."
)
def category_day(day):
    if(day==0 or day>=5):
        return "cat1"
    else:
        return "cat2"
df_train['category_day']=df_train['day'].apply(category_day)
df_test['category_day']=df_test['day'].apply(category_day)

encoded_array = encoder.fit_transform(df_train[['category_day']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category_day']))

df_train = pd.concat([df_train.drop(columns=['category_day']), encoded_df], axis=1)

encoded_array = encoder.transform(df_test[['category_day']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category_day']))

df_test = pd.concat([df_test.drop(columns=['category_day']), encoded_df], axis=1)

df_test['category_day']=df_test['category_day_cat2']
df_test.drop(columns=['category_day_cat2'],inplace=True)

df_train['category_day']=df_train['category_day_cat2']
df_train.drop(columns=['category_day_cat2'],inplace=True)

df_train.category_day.value_counts()

df_fraud = df_train[df_train['is_fraud'] == 1]  # Keep only fraud transactions

ax = sns.histplot(data=df_fraud, x="day", common_norm=False, stat='percent', multiple='dodge')

ax.set_xticklabels(["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"])
ax.set_ylabel('Percentage')
ax.set_xlabel('Day of Week')
plt.title('Fraud Transactions by Day of the Week')
plt.show()

df_train['month']=pd.to_datetime(df_train['trans_date_trans_time']).dt.month
df_test['month']=pd.to_datetime(df_test['trans_date_trans_time']).dt.month
ax=sns.histplot(data=df_train, x="month", hue="is_fraud", common_norm=False,stat='percent',multiple='dodge')
ax.set_ylabel('Percentage')
ax.set_xlabel('Month')
plt.xticks(np.arange(1,13,1))
ax.set_xticklabels(["Jan","Feb","Mar","Apr","May","Jun","Jul",'Aug','Sep','Oct','Nov','Dec'])
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

df_fraud = df_train[df_train['is_fraud'] == 1]  # Keep only fraud transactions

ax = sns.histplot(data=df_fraud, x="month", common_norm=False, stat='percent', multiple='dodge')

ax.set_ylabel('Percentage')
ax.set_xlabel('Month')

plt.xticks(np.arange(1, 13, 1), ["Jan", "Feb", "Mar", "Apr", "May", "Jun",
                                 "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"])

plt.title('Fraud Transactions by Month')
plt.show()

df_train.head()

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np

X_train = df_train.drop(columns=['trans_date_trans_time','day','hour','is_fraud','month']).copy()
y_train = df_train['is_fraud'].copy()

X_test = df_test.drop(columns=['trans_date_trans_time','day','hour','is_fraud','month']).copy()
y_test = df_test['is_fraud'].copy()

#Feature Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)

X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)

class FraudDataset(Dataset):
    def __init__(self, features, labels):
        self.features = features
        self.labels = labels
    def __len__(self):
        return len(self.features)
    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

train_dataset = FraudDataset(X_train_tensor, y_train_tensor)
test_dataset = FraudDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=512, pin_memory=True)

class BinaryClassifier(nn.Module):
    def __init__(self, input_dim):
        super(BinaryClassifier, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(32, 1)
        )
    def forward(self, x):
        return self.model(x)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BinaryClassifier(X_train_tensor.shape[1]).to(device)

criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
# Training Loop
epochs = 20
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for features, labels in train_loader:
        features, labels = features.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(features)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    avg_loss = total_loss / len(train_loader)
    print(f"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}")

#  Evaluation
model.eval()
correct = 0
total = 0
with torch.no_grad():
    for features, labels in test_loader:
        features, labels = features.to(device), labels.to(device)
        outputs = model(features)
        predicted = torch.sigmoid(outputs)
        predicted = (predicted > 0.5).float()
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
accuracy = correct / total
print(f"Test Accuracy: {accuracy:.4f}")

from sklearn.metrics import precision_score, recall_score, f1_score

model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for features, labels in test_loader:
        features = features.to(device)
        outputs = model(features)
        preds = torch.sigmoid(outputs).cpu().numpy()
        all_preds.extend(preds)
        all_labels.extend(labels.numpy())

# Convert predictions to binary labels
# Ensure all_preds is a flat list or array before converting to binary
all_preds_flat = [p for sublist in all_preds for p in (sublist if isinstance(sublist, (list, np.ndarray)) else [sublist])]
binary_preds = [1 if p > 0.5 else 0 for p in all_preds_flat]

# Ensure all_labels is a flat list or array
all_labels_flat = [l for sublist in all_labels for l in (sublist if isinstance(sublist, (list, np.ndarray)) else [sublist])]

# Calculate metrics
precision = precision_score(all_labels_flat, binary_preds)
recall = recall_score(all_labels_flat, binary_preds)
f1 = f1_score(all_labels_flat, binary_preds)

print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")
print(f"F1 Score:  {f1:.4f}")

amt_saved = df_test[(binary_preds==y_test) & (df_test.is_fraud==1)].amt.sum()

amt_saved

fraud_amt = df_test[df_test.is_fraud==1].amt.sum()

fraud_amt

saved_percentage=amt_saved/fraud_amt*100

print(f'Using this model saves {saved_percentage} % of fraud money')

#dly

import gradio as gr
import torch
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import pandas as pd
import joblib

# Load the model components
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Assuming your model and scaler are saved like this after training
# torch.save(model.state_dict(), "fraud_model.pth")
# joblib.dump(scaler, "scaler.pkl")
# joblib.dump(encoder, "encoder.pkl")

torch.save(model.state_dict(), "fraud_model.pth")
joblib.dump(scaler, "scaler.pkl")
joblib.dump(encoder, "encoder.pkl")

class BinaryClassifier(torch.nn.Module):
    def __init__(self, input_dim):
        super(BinaryClassifier, self).__init__()
        self.model = torch.nn.Sequential(
            torch.nn.Linear(input_dim, 128),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(128, 64),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(64, 32),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(32, 1)
        )
    def forward(self, x):
        return self.model(x)

# Load scaler and encoders
scaler = joblib.load("scaler.pkl")
encoder = joblib.load("encoder.pkl")

# Instantiate and load model
input_dim = scaler.transform(np.zeros((1, scaler.mean_.shape[0]))).shape[1]
model = BinaryClassifier(input_dim)
model.load_state_dict(torch.load("fraud_model.pth", map_location=device))
model.to(device)
model.eval()

# Define prediction function
def predict_fraud(category, gender, amt, hour, day, month):
    # Preprocessing
    gender = 1 if gender == "M" else 0
    is_amt_greater_than_200 = 1 if amt >= 200 else 0
    time_category = "night" if (hour >= 22 or hour <= 3) else "day"
    category_day = "cat1" if (day == 0 or day >= 5) else "cat2"

    # One-hot encode category features
    category_encoded = encoder.transform([[category]]).flatten()
    time_cat_encoded = encoder.transform([[time_category]]).flatten()
    cat_day_encoded = encoder.transform([[category_day]]).flatten()

    # Prepare full feature vector
    input_features = np.concatenate([
        [amt, gender, is_amt_greater_than_200, hour, day, month],
        category_encoded,
        time_cat_encoded,
        cat_day_encoded
    ])

    # Feature scaling
    input_scaled = scaler.transform([input_features])
    input_tensor = torch.tensor(input_scaled, dtype=torch.float32).to(device)

    with torch.no_grad():
        output = torch.sigmoid(model(input_tensor))
        prediction = output.item()

    return {"Fraud Probability": round(prediction, 4), "Prediction": "Fraud" if prediction > 0.5 else "Not Fraud"}

# Gradio UI
category_options = ['grocery_pos', 'shopping_net', 'misc_net', 'shopping_pos', 'gas_transport', 'others']
interface = gr.Interface(
    fn=predict_fraud,
    inputs=[
        gr.Dropdown(category_options, label="Category"),
        gr.Radio(["M", "F"], label="Gender"),
        gr.Number(label="Transaction Amount (USD)"),
        gr.Slider(0, 23, step=1, label="Hour of Transaction"),
        gr.Slider(0, 6, step=1, label="Day of Week (0=Mon, 6=Sun)"),
        gr.Slider(1, 12, step=1, label="Month")
    ],
    outputs=[
        gr.Number(label="Fraud Probability"),
        gr.Text(label="Prediction")
    ],
    title="Fraud Detection Model",
    description="Enter transaction details to predict if it's fraudulent."
)

interface.launch()

encoded_array = encoder.fit_transform(df_train[['category_day']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category_day']))

df_train = pd.concat([df_train.drop(columns=['category_day']), encoded_df], axis=1)

encoded_array = encoder.transform(df_test[['category_day']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category_day']))

df_test = pd.concat([df_test.drop(columns=['category_day']), encoded_df], axis=1)

df_test['category_day']=df_test['category_day_cat2']
df_test.drop(columns=['category_day_cat2'],inplace=True)

df_train['category_day']=df_train['category_day_cat2']
df_train.drop(columns=['category_day_cat2'],inplace=True)

def category_day(day):
    if(day==0 or day>=5):
        return "cat1"
    else:
        return "cat2"
df_train['category_day']=df_train['day'].apply(category_day)
df_test['category_day']=df_test['day'].apply(category_day)

encoded_array = encoder.fit_transform(df_train[['category_day']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category_day']))

df_train = pd.concat([df_train.drop(columns=['category_day']), encoded_df], axis=1)

encoded_array = encoder.transform(df_test[['category_day']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category_day']))

df_test = pd.concat([df_test.drop(columns=['category_day']), encoded_df], axis=1)

df_test['category_day']=df_test['category_day_cat2']
df_test.drop(columns=['category_day_cat2'],inplace=True)

df_train['category_day']=df_train['category_day_cat2']
df_train.drop(columns=['category_day_cat2'],inplace=True)

df_train.category_day.value_counts()

df_fraud = df_train[df_train['is_fraud'] == 1]  # Keep only fraud transactions

ax = sns.histplot(data=df_fraud, x="day", common_norm=False, stat='percent', multiple='dodge')

ax.set_xticklabels(["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"])
ax.set_ylabel('Percentage')
ax.set_xlabel('Day of Week')
plt.title('Fraud Transactions by Day of the Week')
plt.show()

df_train['month']=pd.to_datetime(df_train['trans_date_trans_time']).dt.month
df_test['month']=pd.to_datetime(df_test['trans_date_trans_time']).dt.month
ax=sns.histplot(data=df_train, x="month", hue="is_fraud", common_norm=False,stat='percent',multiple='dodge')
ax.set_ylabel('Percentage')
ax.set_xlabel('Month')
plt.xticks(np.arange(1,13,1))
ax.set_xticklabels(["Jan","Feb","Mar","Apr","May","Jun","Jul",'Aug','Sep','Oct','Nov','Dec'])
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

df_fraud = df_train[df_train['is_fraud'] == 1]  # Keep only fraud transactions

ax = sns.histplot(data=df_fraud, x="month", common_norm=False, stat='percent', multiple='dodge')

ax.set_ylabel('Percentage')
ax.set_xlabel('Month')

plt.xticks(np.arange(1, 13, 1), ["Jan", "Feb", "Mar", "Apr", "May", "Jun",
                                 "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"])

plt.title('Fraud Transactions by Month')
plt.show()

df_train.head()

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np

X_train = df_train.drop(columns=['trans_date_trans_time','day','hour','is_fraud','month']).copy()
y_train = df_train['is_fraud'].copy()

X_test = df_test.drop(columns=['trans_date_trans_time','day','hour','is_fraud','month']).copy()
y_test = df_test['is_fraud'].copy()

#Feature Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)

X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)

class FraudDataset(Dataset):
    def __init__(self, features, labels):
        self.features = features
        self.labels = labels
    def __len__(self):
        return len(self.features)
    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

train_dataset = FraudDataset(X_train_tensor, y_train_tensor)
test_dataset = FraudDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=512, pin_memory=True)

class BinaryClassifier(nn.Module):
    def __init__(self, input_dim):
        super(BinaryClassifier, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(32, 1)
        )
    def forward(self, x):
        return self.model(x)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BinaryClassifier(X_train_tensor.shape[1]).to(device)

criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
# Training Loop
epochs = 20
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for features, labels in train_loader:
        features, labels = features.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(features)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    avg_loss = total_loss / len(train_loader)
    print(f"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}")

#  Evaluation
model.eval()
correct = 0
total = 0
with torch.no_grad():
    for features, labels in test_loader:
        features, labels = features.to(device), labels.to(device)
        outputs = model(features)
        predicted = torch.sigmoid(outputs)
        predicted = (predicted > 0.5).float()
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
accuracy = correct / total
print(f"Test Accuracy: {accuracy:.4f}")

from sklearn.metrics import precision_score, recall_score, f1_score

model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for features, labels in test_loader:
        features = features.to(device)
        outputs = model(features)
        preds = torch.sigmoid(outputs).cpu().numpy()
        all_preds.extend(preds)
        all_labels.extend(labels.numpy())

# Convert predictions to binary labels
# Ensure all_preds is a flat list or array before converting to binary
all_preds_flat = [p for sublist in all_preds for p in (sublist if isinstance(sublist, (list, np.ndarray)) else [sublist])]
binary_preds = [1 if p > 0.5 else 0 for p in all_preds_flat]

# Ensure all_labels is a flat list or array
all_labels_flat = [l for sublist in all_labels for l in (sublist if isinstance(sublist, (list, np.ndarray)) else [sublist])]

# Calculate metrics
precision = precision_score(all_labels_flat, binary_preds)
recall = recall_score(all_labels_flat, binary_preds)
f1 = f1_score(all_labels_flat, binary_preds)

print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")
print(f"F1 Score:  {f1:.4f}")

amt_saved = df_test[(binary_preds==y_test) & (df_test.is_fraud==1)].amt.sum()

amt_saved

fraud_amt = df_test[df_test.is_fraud==1].amt.sum()

fraud_amt

saved_percentage=amt_saved/fraud_amt*100

print(f'Using this model saves {saved_percentage} % of fraud money')

#dly

import gradio as gr
import torch
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import pandas as pd
import joblib

# Load the model components
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Assuming your model and scaler are saved like this after training
# torch.save(model.state_dict(), "fraud_model.pth")
# joblib.dump(scaler, "scaler.pkl")
# joblib.dump(encoder, "encoder.pkl")

torch.save(model.state_dict(), "fraud_model.pth")
joblib.dump(scaler, "scaler.pkl")
joblib.dump(encoder, "encoder.pkl")

class BinaryClassifier(torch.nn.Module):
    def __init__(self, input_dim):
        super(BinaryClassifier, self).__init__()
        self.model = torch.nn.Sequential(
            torch.nn.Linear(input_dim, 128),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(128, 64),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(64, 32),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(32, 1)
        )
    def forward(self, x):
        return self.model(x)

# Load scaler and encoders
scaler = joblib.load("scaler.pkl")
encoder = joblib.load("encoder.pkl")

# Instantiate and load model
input_dim = scaler.transform(np.zeros((1, scaler.mean_.shape[0]))).shape[1]
model = BinaryClassifier(input_dim)
model.load_state_dict(torch.load("fraud_model.pth", map_location=device))
model.to(device)
model.eval()

# Define prediction function
def predict_fraud(category, gender, amt, hour, day, month):
    # Preprocessing
    gender = 1 if gender == "M" else 0
    is_amt_greater_than_200 = 1 if amt >= 200 else 0
    time_category = "night" if (hour >= 22 or hour <= 3) else "day"
    category_day = "cat1" if (day == 0 or day >= 5) else "cat2"

    # One-hot encode category features
    category_encoded = encoder.transform([[category]]).flatten()
    time_cat_encoded = encoder.transform([[time_category]]).flatten()
    cat_day_encoded = encoder.transform([[category_day]]).flatten()

    # Prepare full feature vector
    input_features = np.concatenate([
        [amt, gender, is_amt_greater_than_200, hour, day, month],
        category_encoded,
        time_cat_encoded,
        cat_day_encoded
    ])

    # Feature scaling
    input_scaled = scaler.transform([input_features])
    input_tensor = torch.tensor(input_scaled, dtype=torch.float32).to(device)

    with torch.no_grad():
        output = torch.sigmoid(model(input_tensor))
        prediction = output.item()

    return {"Fraud Probability": round(prediction, 4), "Prediction": "Fraud" if prediction > 0.5 else "Not Fraud"}

# Gradio UI
category_options = ['grocery_pos', 'shopping_net', 'misc_net', 'shopping_pos', 'gas_transport', 'others']
interface = gr.Interface(
    fn=predict_fraud,
    inputs=[
        gr.Dropdown(category_options, label="Category"),
        gr.Radio(["M", "F"], label="Gender"),
        gr.Number(label="Transaction Amount (USD)"),
        gr.Slider(0, 23, step=1, label="Hour of Transaction"),
        gr.Slider(0, 6, step=1, label="Day of Week (0=Mon, 6=Sun)"),
        gr.Slider(1, 12, step=1, label="Month")
    ],
    outputs=[
        gr.Number(label="Fraud Probability"),
        gr.Text(label="Prediction")
    ],
    title="Fraud Detection Model",
    description="Enter transaction details to predict if it's fraudulent."
)

interface.launch()

ax.set_xticklabels(["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"])
ax.set_ylabel('Percentage')
ax.set_xlabel('Day of Week')
plt.title('Fraud Transactions by Day of the Week')
plt.show()

df_train['month']=pd.to_datetime(df_train['trans_date_trans_time']).dt.month
df_test['month']=pd.to_datetime(df_test['trans_date_trans_time']).dt.month
ax=sns.histplot(data=df_train, x="month", hue="is_fraud", common_norm=False,stat='percent',multiple='dodge')
ax.set_ylabel('Percentage')
ax.set_xlabel('Month')
plt.xticks(np.arange(1,13,1))
ax.set_xticklabels(["Jan","Feb","Mar","Apr","May","Jun","Jul",'Aug','Sep','Oct','Nov','Dec'])
#dly

import gradio as gr
import torch
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import pandas as pd
import joblib

# Load the model components
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Assuming your model and scaler are saved like this after training
# torch.save(model.state_dict(), "fraud_model.pth")
# joblib.dump(scaler, "scaler.pkl")
# joblib.dump(encoder, "encoder.pkl")

torch.save(model.state_dict(), "fraud_model.pth")
joblib.dump(scaler, "scaler.pkl")
joblib.dump(encoder, "encoder.pkl")

class BinaryClassifier(torch.nn.Module):
    def __init__(self, input_dim):
        super(BinaryClassifier, self).__init__()
        self.model = torch.nn.Sequential(
            torch.nn.Linear(input_dim, 128),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(128, 64),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(64, 32),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(32, 1)
        )
    def forward(self, x):
        return self.model(x)

# Load scaler and encoders
scaler = joblib.load("scaler.pkl")
encoder = joblib.load("encoder.pkl")

# Instantiate and load model
input_dim = scaler.transform(np.zeros((1, scaler.mean_.shape[0]))).shape[1]
model = BinaryClassifier(input_dim)
model.load_state_dict(torch.load("fraud_model.pth", map_location=device))
model.to(device)
model.eval()

# Define prediction function
def predict_fraud(category, gender, amt, hour, day, month):
    # Preprocessing
    gender = 1 if gender == "M" else 0
    is_amt_greater_than_200 = 1 if amt >= 200 else 0
    time_category = "night" if (hour >= 22 or hour <= 3) else "day"
    category_day = "cat1" if (day == 0 or day >= 5) else "cat2"

    # One-hot encode category features
    category_encoded = encoder.transform([[category]]).flatten()
    time_cat_encoded = encoder.transform([[time_category]]).flatten()
    cat_day_encoded = encoder.transform([[category_day]]).flatten()

    # Prepare full feature vector
    input_features = np.concatenate([
        [amt, gender, is_amt_greater_than_200, hour, day, month],
        category_encoded,
        time_cat_encoded,
        cat_day_encoded
    ])

    # Feature scaling
    input_scaled = scaler.transform([input_features])
    input_tensor = torch.tensor(input_scaled, dtype=torch.float32).to(device)

    with torch.no_grad():
        output = torch.sigmoid(model(input_tensor))
        prediction = output.item()

    return {"Fraud Probability": round(prediction, 4), "Prediction": "Fraud" if prediction > 0.5 else "Not Fraud"}

# Gradio UI
category_options = ['grocery_pos', 'shopping_net', 'misc_net', 'shopping_pos', 'gas_transport', 'others']
interface = gr.Interface(
    fn=predict_fraud,
    inputs=[
        gr.Dropdown(category_options, label="Category"),
        gr.Radio(["M", "F"], label="Gender"),
        gr.Number(label="Transaction Amount (USD)"),
        gr.Slider(0, 23, step=1, label="Hour of Transaction"),
        gr.Slider(0, 6, step=1, label="Day of Week (0=Mon, 6=Sun)"),
        gr.Slider(1, 12, step=1, label="Month")
    ],
    outputs=[
        gr.Number(label="Fraud Probability"),
        gr.Text(label="Prediction")
    ],
    title="Fraud Detection Model",
    description="Enter transaction details to predict if it's fraudulent."
)

interface.launch()

ax.set_ylabel('Percentage')
ax.set_xlabel('Month')
def M_1_F_0(instance):
    if(instance=='M'):
        return 1
    return 0

df_train['gender']=df_train['gender'].apply(M_1_F_0)
df_test['gender']=df_test['gender'].apply(M_1_F_0)

df_train.street.value_counts()

df_train[df_train.is_fraud==1].city.value_counts()

df_train['zip'].value_counts()

df_train.drop(columns=['street','city','state','zip','dob','trans_num','city_pop'],inplace=True)
df_test.drop(columns=['street','city','state','zip','dob','trans_num','city_pop'],inplace=True)
df_train.head()

df_train.drop(columns=['unix_time'],inplace=True)
df_test.drop(columns=['unix_time'],inplace=True)

df_train.head()

df_train.job.value_counts()

df_train[df_train.is_fraud==1].job.value_counts()

(62+56+53+51+50)/(len(df_train[df_train.is_fraud==1]))*100

df_train.drop(columns=['job'],inplace=True)
df_test.drop(columns=['job'],inplace=True)
df_train.head()

ax=sns.histplot(x='amt',data=df_train[df_train.amt<=1000],hue='is_fraud',stat='percent',multiple='dodge',common_norm=False,bins=25)
ax.set_ylabel('Percentage in Each Type')
ax.set_xlabel('Transaction Amount in USD')
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

df_train[(df_train.amt>=200)& (df_train.is_fraud==1)].amt.sum()/df_train[df_train.is_fraud==1].amt.sum()*100
plt.show()

df_train.head()

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np

X_train = df_train.drop(columns=['trans_date_trans_time','day','hour','is_fraud','month']).copy()
y_train = df_train['is_fraud'].copy()

X_test = df_test.drop(columns=['trans_date_trans_time','day','hour','is_fraud','month']).copy()
y_test = df_test['is_fraud'].copy()

#Feature Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)

X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)
!pip install gradio

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df_train = pd.read_csv('fraudTrain.csv', index_col=0)
df_test = pd.read_csv('fraudTest.csv',index_col=0)

df_train.head()

df_train.columns

row,col=df_train.shape
print(f'The dataset has {row} rows and {col} columns.')

print(df_train.describe())

df_train.info()

fraud_percentage=len(df_train[df_train['is_fraud']==1])/len(df_train)*100
non_fraud_percentage=100-fraud_percentage
print(f'The percentage of fraud is {fraud_percentage} % and the percentage of non-fraud is {non_fraud_percentage} %')
print('This shows how imbalanced is the dataset')

df_train.isna().sum()

len(df_train)==len(df_train.drop_duplicates())

df_train.head()

df_train['cc_num'].value_counts()

df_train[df_train['is_fraud']==1].cc_num.value_counts()

df_train.drop(columns=['cc_num'],inplace=True)
df_test.drop(columns=['cc_num'],inplace=True)

df_train.merchant.value_counts()

df_train[df_train['is_fraud']==1].merchant.value_counts().head(50)

df_train.drop(columns=['merchant'],inplace=True)
df_test.drop(columns=['merchant'],inplace=True)

df_train.category.value_counts()

df_train[df_train['is_fraud']==1].category.value_counts()

(1743+1713+915+843+618)/(df_train[df_train['is_fraud']==1].category.value_counts().sum())*100

df_train.groupby('category')['amt'].sum().sort_values(ascending=False)

(14460822.38+9307993.61+8625149.68+8351732.29+7173928.11)/(df_train.groupby('category')['amt'].sum().sum())*100

def convert_to_other(instance):
    if(instance in ['grocery_pos', 'shopping_net', 'misc_net','shopping_pos', 'gas_transport']):
        return instance
    else :
        return 'others'
df_train['category']=df_train['category'].apply(convert_to_other)
df_test['category']=df_test['category'].apply(convert_to_other)
df_train.category.value_counts()

df_train.head()

from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(drop='first', sparse_output=False)
# drop='first' removes one column to avoid redundancy which also remove multicollinearity
# if we don't remove the input feature will have depencies. sum of values in the col will be 1 which can effect the performance

encoded_array = encoder.fit_transform(df_train[['category']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category']))

df_train = pd.concat([df_train.drop(columns=['category']), encoded_df], axis=1)

encoded_array = encoder.transform(df_test[['category']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category']))

df_test = pd.concat([df_test.drop(columns=['category']), encoded_df], axis=1)

df_train.drop(columns=['first','last'],inplace=True)
df_test.drop(columns=['first','last'],inplace=True)

def M_1_F_0(instance):
    if(instance=='M'):
        return 1
    return 0

df_train['gender']=df_train['gender'].apply(M_1_F_0)
df_test['gender']=df_test['gender'].apply(M_1_F_0)

df_train.street.value_counts()

df_train[df_train.is_fraud==1].city.value_counts()

df_train['zip'].value_counts()

df_train.drop(columns=['street','city','state','zip','dob','trans_num','city_pop'],inplace=True)
df_test.drop(columns=['street','city','state','zip','dob','trans_num','city_pop'],inplace=True)
df_train.head()

df_train.drop(columns=['unix_time'],inplace=True)
df_test.drop(columns=['unix_time'],inplace=True)

df_train.head()

df_train.job.value_counts()

df_train[df_train.is_fraud==1].job.value_counts()

(62+56+53+51+50)/(len(df_train[df_train.is_fraud==1]))*100

df_train.drop(columns=['job'],inplace=True)
df_test.drop(columns=['job'],inplace=True)
df_train.head()

ax=sns.histplot(x='amt',data=df_train[df_train.amt<=1000],hue='is_fraud',stat='percent',multiple='dodge',common_norm=False,bins=25)
ax.set_ylabel('Percentage in Each Type')
ax.set_xlabel('Transaction Amount in USD')
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

df_train[(df_train.amt>=200)& (df_train.is_fraud==1)].amt.sum()/df_train[df_train.is_fraud==1].amt.sum()*100

def is_greater_than_200(amt):
    if(amt>=200):
        return 1
    return 0
df_train['is_amt_greater_than_200']=df_train.amt.apply(is_greater_than_200)
df_test['is_amt_greater_than_200']=df_test.amt.apply(is_greater_than_200)

split_sum = df_train[df_train.is_fraud==1].groupby('is_amt_greater_than_200')['amt'].sum()

plt.bar(split_sum.index.astype(str), split_sum.values, color=['blue', 'orange'])

df_train['hour']=pd.to_datetime(df_train['trans_date_trans_time']).dt.hour
df_test['hour']=pd.to_datetime(df_test['trans_date_trans_time']).dt.hour
ax=sns.histplot(data=df_train, x="hour", hue="is_fraud", common_norm=False,stat='percent',multiple='dodge')
ax.set_ylabel('Percentage')
ax.set_xlabel('Time (Hour) in a Day')
plt.xticks(np.arange(0,24,1))
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

df_train[(df_train.is_fraud == 1) & ((df_train.hour <= 3) | (df_train.hour >= 22))]['amt'].sum()/(df_train[df_train.is_fraud==1].amt.sum())*100

def time_category(hour):
    if hour>=22 or hour<=3 :
        return "night"
    else :
        return "day"

df_train['time_category']= df_train['hour'].apply(time_category)
df_test['time_category']= df_test['hour'].apply(time_category)

df_train.time_category.value_counts()

df_train.info()

encoder = OneHotEncoder(drop='first', sparse_output=False)
encoded_array = encoder.fit_transform(df_train[['time_category']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['time_category']))

df_train = pd.concat([df_train.drop(columns=['time_category']), encoded_df], axis=1)

encoded_array = encoder.transform(df_test[['time_category']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['time_category']))

df_test = pd.concat([df_test.drop(columns=['time_category']), encoded_df], axis=1)

df_train.info()

df_train.head()

df_train['day']=pd.to_datetime(df_train['trans_date_trans_time']).dt.dayofweek
df_test['day']=pd.to_datetime(df_test['trans_date_trans_time']).dt.dayofweek
ax=sns.histplot(data=df_train, x="day", hue="is_fraud", common_norm=False,stat='percent',multiple='dodge')
#ax.set_xticklabels(['',"Mon","Tue","Wed","Thu","Fri","Sat","Sun"])
plt.xticks(np.arange(7), ["Mon","Tue","Wed","Thu","Fri","Sat","Sun"]) #set the xtick labels
ax.set_ylabel('Percentage')
ax.set_xlabel('Day of Week')
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

df_train[(df_train.is_fraud == 1) & ((df_train.day >= 5)|(df_train.day==0))]['amt'].sum() / df_train[df_train.is_fraud == 1]['amt'].sum()*100

def category_day(day):
    if(day==0 or day>=5):
        return "cat1"
    else:
        return "cat2"
df_train['category_day']=df_train['day'].apply(category_day)
df_test['category_day']=df_test['day'].apply(category_day)

encoded_array = encoder.fit_transform(df_train[['category_day']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category_day']))

df_train = pd.concat([df_train.drop(columns=['category_day']), encoded_df], axis=1)

encoded_array = encoder.transform(df_test[['category_day']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category_day']))

df_test = pd.concat([df_test.drop(columns=['category_day']), encoded_df], axis=1)

df_test['category_day']=df_test['category_day_cat2']
df_test.drop(columns=['category_day_cat2'],inplace=True)

df_train['category_day']=df_train['category_day_cat2']
df_train.drop(columns=['category_day_cat2'],inplace=True)

df_train.category_day.value_counts()

df_fraud = df_train[df_train['is_fraud'] == 1]  # Keep only fraud transactions

ax = sns.histplot(data=df_fraud, x="day", common_norm=False, stat='percent', multiple='dodge')

ax.set_xticklabels(["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"])
ax.set_ylabel('Percentage')
ax.set_xlabel('Day of Week')
plt.title('Fraud Transactions by Day of the Week')
plt.show()

df_train['month']=pd.to_datetime(df_train['trans_date_trans_time']).dt.month
df_test['month']=pd.to_datetime(df_test['trans_date_trans_time']).dt.month
ax=sns.histplot(data=df_train, x="month", hue="is_fraud", common_norm=False,stat='percent',multiple='dodge')
ax.set_ylabel('Percentage')
ax.set_xlabel('Month')
plt.xticks(np.arange(1,13,1))
ax.set_xticklabels(["Jan","Feb","Mar","Apr","May","Jun","Jul",'Aug','Sep','Oct','Nov','Dec'])
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

df_fraud = df_train[df_train['is_fraud'] == 1]  # Keep only fraud transactions

ax = sns.histplot(data=df_fraud, x="month", common_norm=False, stat='percent', multiple='dodge')

ax.set_ylabel('Percentage')
ax.set_xlabel('Month')

plt.xticks(np.arange(1, 13, 1), ["Jan", "Feb", "Mar", "Apr", "May", "Jun",
                                 "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"])

plt.title('Fraud Transactions by Month')
plt.show()

df_train.head()

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np

X_train = df_train.drop(columns=['trans_date_trans_time','day','hour','is_fraud','month']).copy()
y_train = df_train['is_fraud'].copy()

X_test = df_test.drop(columns=['trans_date_trans_time','day','hour','is_fraud','month']).copy()
y_test = df_test['is_fraud'].copy()

#Feature Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)

X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)

class FraudDataset(Dataset):
    def __init__(self, features, labels):
        self.features = features
        self.labels = labels
    def __len__(self):
        return len(self.features)
    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

train_dataset = FraudDataset(X_train_tensor, y_train_tensor)
test_dataset = FraudDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=512, pin_memory=True)

class BinaryClassifier(nn.Module):
    def __init__(self, input_dim):
        super(BinaryClassifier, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(32, 1)
        )
    def forward(self, x):
        return self.model(x)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BinaryClassifier(X_train_tensor.shape[1]).to(device)

criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
# Training Loop
epochs = 20
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for features, labels in train_loader:
        features, labels = features.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(features)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    avg_loss = total_loss / len(train_loader)
    print(f"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}")

#  Evaluation
model.eval()
correct = 0
total = 0
with torch.no_grad():
    for features, labels in test_loader:
        features, labels = features.to(device), labels.to(device)
        outputs = model(features)
        predicted = torch.sigmoid(outputs)
        predicted = (predicted > 0.5).float()
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
accuracy = correct / total
print(f"Test Accuracy: {accuracy:.4f}")

from sklearn.metrics import precision_score, recall_score, f1_score

model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for features, labels in test_loader:
        features = features.to(device)
        outputs = model(features)
        preds = torch.sigmoid(outputs).cpu().numpy()
        all_preds.extend(preds)
        all_labels.extend(labels.numpy())

# Convert predictions to binary labels
# Ensure all_preds is a flat list or array before converting to binary
all_preds_flat = [p for sublist in all_preds for p in (sublist if isinstance(sublist, (list, np.ndarray)) else [sublist])]
binary_preds = [1 if p > 0.5 else 0 for p in all_preds_flat]

# Ensure all_labels is a flat list or array
all_labels_flat = [l for sublist in all_labels for l in (sublist if isinstance(sublist, (list, np.ndarray)) else [sublist])]

# Calculate metrics
precision = precision_score(all_labels_flat, binary_preds)
recall = recall_score(all_labels_flat, binary_preds)
f1 = f1_score(all_labels_flat, binary_preds)

print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")
print(f"F1 Score:  {f1:.4f}")

amt_saved = df_test[(binary_preds==y_test) & (df_test.is_fraud==1)].amt.sum()

amt_saved

fraud_amt = df_test[df_test.is_fraud==1].amt.sum()

fraud_amt

saved_percentage=amt_saved/fraud_amt*100

print(f'Using this model saves {saved_percentage} % of fraud money')

#dly

import gradio as gr
import torch
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import pandas as pd
import joblib

# Load the model components
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Assuming your model and scaler are saved like this after training
# torch.save(model.state_dict(), "fraud_model.pth")
# joblib.dump(scaler, "scaler.pkl")
# joblib.dump(encoder, "encoder.pkl")

torch.save(model.state_dict(), "fraud_model.pth")
joblib.dump(scaler, "scaler.pkl")
joblib.dump(encoder, "encoder.pkl")

class BinaryClassifier(torch.nn.Module):
    def __init__(self, input_dim):
        super(BinaryClassifier, self).__init__()
        self.model = torch.nn.Sequential(
            torch.nn.Linear(input_dim, 128),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(128, 64),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(64, 32),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(32, 1)
        )
    def forward(self, x):
        return self.model(x)

# Load scaler and encoders
scaler = joblib.load("scaler.pkl")
encoder = joblib.load("encoder.pkl")

# Instantiate and load model
input_dim = scaler.transform(np.zeros((1, scaler.mean_.shape[0]))).shape[1]
model = BinaryClassifier(input_dim)
model.load_state_dict(torch.load("fraud_model.pth", map_location=device))
model.to(device)
model.eval()

# Define prediction function
def predict_fraud(category, gender, amt, hour, day, month):
    # Preprocessing
    gender = 1 if gender == "M" else 0
    is_amt_greater_than_200 = 1 if amt >= 200 else 0
    time_category = "night" if (hour >= 22 or hour <= 3) else "day"
    category_day = "cat1" if (day == 0 or day >= 5) else "cat2"

    # One-hot encode category features
    category_encoded = encoder.transform([[category]]).flatten()
    time_cat_encoded = encoder.transform([[time_category]]).flatten()
    cat_day_encoded = encoder.transform([[category_day]]).flatten()

    # Prepare full feature vector
    input_features = np.concatenate([
        [amt, gender, is_amt_greater_than_200, hour, day, month],
        category_encoded,
        time_cat_encoded,
        cat_day_encoded
    ])

    # Feature scaling
    input_scaled = scaler.transform([input_features])
    input_tensor = torch.tensor(input_scaled, dtype=torch.float32).to(device)

    with torch.no_grad():
        output = torch.sigmoid(model(input_tensor))
        prediction = output.item()

    return {"Fraud Probability": round(prediction, 4), "Prediction": "Fraud" if prediction > 0.5 else "Not Fraud"}

# Gradio UI
category_options = ['grocery_pos', 'shopping_net', 'misc_net', 'shopping_pos', 'gas_transport', 'others']
interface = gr.Interface(
    fn=predict_fraud,
    inputs=[
        gr.Dropdown(category_options, label="Category"),
        gr.Radio(["M", "F"], label="Gender"),
        gr.Number(label="Transaction Amount (USD)"),
        gr.Slider(0, 23, step=1, label="Hour of Transaction"),
        gr.Slider(0, 6, step=1, label="Day of Week (0=Mon, 6=Sun)"),
        gr.Slider(1, 12, step=1, label="Month")
    ],
    outputs=[
        gr.Number(label="Fraud Probability"),
        gr.Text(label="Prediction")
    ],
    title="Fraud Detection Model",
    description="Enter transaction details to predict if it's fraudulent."
)
!pip install gradio

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df_train = pd.read_csv('fraudTrain.csv', index_col=0)
df_test = pd.read_csv('fraudTest.csv',index_col=0)

df_train.head()

df_train.columns

row,col=df_train.shape
print(f'The dataset has {row} rows and {col} columns.')

print(df_train.describe())

df_train.info()

fraud_percentage=len(df_train[df_train['is_fraud']==1])/len(df_train)*100
non_fraud_percentage=100-fraud_percentage
print(f'The percentage of fraud is {fraud_percentage} % and the percentage of non-fraud is {non_fraud_percentage} %')
print('This shows how imbalanced is the dataset')

df_train.isna().sum()

len(df_train)==len(df_train.drop_duplicates())

df_train.head()

df_train['cc_num'].value_counts()

df_train[df_train['is_fraud']==1].cc_num.value_counts()

df_train.drop(columns=['cc_num'],inplace=True)
df_test.drop(columns=['cc_num'],inplace=True)

df_train.merchant.value_counts()

df_train[df_train['is_fraud']==1].merchant.value_counts().head(50)

df_train.drop(columns=['merchant'],inplace=True)
df_test.drop(columns=['merchant'],inplace=True)

df_train.category.value_counts()

df_train[df_train['is_fraud']==1].category.value_counts()

(1743+1713+915+843+618)/(df_train[df_train['is_fraud']==1].category.value_counts().sum())*100

df_train.groupby('category')['amt'].sum().sort_values(ascending=False)

(14460822.38+9307993.61+8625149.68+8351732.29+7173928.11)/(df_train.groupby('category')['amt'].sum().sum())*100

def convert_to_other(instance):
    if(instance in ['grocery_pos', 'shopping_net', 'misc_net','shopping_pos', 'gas_transport']):
        return instance
    else :
        return 'others'
df_train['category']=df_train['category'].apply(convert_to_other)
df_test['category']=df_test['category'].apply(convert_to_other)
df_train.category.value_counts()

df_train.head()

from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(drop='first', sparse_output=False)
# drop='first' removes one column to avoid redundancy which also remove multicollinearity
# if we don't remove the input feature will have depencies. sum of values in the col will be 1 which can effect the performance

encoded_array = encoder.fit_transform(df_train[['category']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category']))

df_train = pd.concat([df_train.drop(columns=['category']), encoded_df], axis=1)

encoded_array = encoder.transform(df_test[['category']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category']))

df_test = pd.concat([df_test.drop(columns=['category']), encoded_df], axis=1)

df_train.drop(columns=['first','last'],inplace=True)
df_test.drop(columns=['first','last'],inplace=True)

def M_1_F_0(instance):
    if(instance=='M'):
        return 1
    return 0

df_train['gender']=df_train['gender'].apply(M_1_F_0)
df_test['gender']=df_test['gender'].apply(M_1_F_0)

df_train.street.value_counts()

df_train[df_train.is_fraud==1].city.value_counts()

df_train['zip'].value_counts()

df_train.drop(columns=['street','city','state','zip','dob','trans_num','city_pop'],inplace=True)
df_test.drop(columns=['street','city','state','zip','dob','trans_num','city_pop'],inplace=True)
df_train.head()

df_train.drop(columns=['unix_time'],inplace=True)
df_test.drop(columns=['unix_time'],inplace=True)

df_train.head()

df_train.job.value_counts()

df_train[df_train.is_fraud==1].job.value_counts()

(62+56+53+51+50)/(len(df_train[df_train.is_fraud==1]))*100

df_train.drop(columns=['job'],inplace=True)
df_test.drop(columns=['job'],inplace=True)
df_train.head()

ax=sns.histplot(x='amt',data=df_train[df_train.amt<=1000],hue='is_fraud',stat='percent',multiple='dodge',common_norm=False,bins=25)
ax.set_ylabel('Percentage in Each Type')
ax.set_xlabel('Transaction Amount in USD')
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

df_train[(df_train.amt>=200)& (df_train.is_fraud==1)].amt.sum()/df_train[df_train.is_fraud==1].amt.sum()*100

def is_greater_than_200(amt):
    if(amt>=200):
        return 1
    return 0
df_train['is_amt_greater_than_200']=df_train.amt.apply(is_greater_than_200)
df_test['is_amt_greater_than_200']=df_test.amt.apply(is_greater_than_200)

split_sum = df_train[df_train.is_fraud==1].groupby('is_amt_greater_than_200')['amt'].sum()

plt.bar(split_sum.index.astype(str), split_sum.values, color=['blue', 'orange'])

df_train['hour']=pd.to_datetime(df_train['trans_date_trans_time']).dt.hour
df_test['hour']=pd.to_datetime(df_test['trans_date_trans_time']).dt.hour
ax=sns.histplot(data=df_train, x="hour", hue="is_fraud", common_norm=False,stat='percent',multiple='dodge')
ax.set_ylabel('Percentage')
ax.set_xlabel('Time (Hour) in a Day')
plt.xticks(np.arange(0,24,1))
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

df_train[(df_train.is_fraud == 1) & ((df_train.hour <= 3) | (df_train.hour >= 22))]['amt'].sum()/(df_train[df_train.is_fraud==1].amt.sum())*100

def time_category(hour):
    if hour>=22 or hour<=3 :
        return "night"
    else :
        return "day"

df_train['time_category']= df_train['hour'].apply(time_category)
df_test['time_category']= df_test['hour'].apply(time_category)

df_train.time_category.value_counts()

df_train.info()

encoder = OneHotEncoder(drop='first', sparse_output=False)
encoded_array = encoder.fit_transform(df_train[['time_category']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['time_category']))

df_train = pd.concat([df_train.drop(columns=['time_category']), encoded_df], axis=1)

encoded_array = encoder.transform(df_test[['time_category']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['time_category']))

df_test = pd.concat([df_test.drop(columns=['time_category']), encoded_df], axis=1)

df_train.info()

df_train.head()

df_train['day']=pd.to_datetime(df_train['trans_date_trans_time']).dt.dayofweek
df_test['day']=pd.to_datetime(df_test['trans_date_trans_time']).dt.dayofweek
ax=sns.histplot(data=df_train, x="day", hue="is_fraud", common_norm=False,stat='percent',multiple='dodge')
#ax.set_xticklabels(['',"Mon","Tue","Wed","Thu","Fri","Sat","Sun"])
plt.xticks(np.arange(7), ["Mon","Tue","Wed","Thu","Fri","Sat","Sun"]) #set the xtick labels
ax.set_ylabel('Percentage')
ax.set_xlabel('Day of Week')
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

df_train[(df_train.is_fraud == 1) & ((df_train.day >= 5)|(df_train.day==0))]['amt'].sum() / df_train[df_train.is_fraud == 1]['amt'].sum()*100

def category_day(day):
    if(day==0 or day>=5):
        return "cat1"
    else:
        return "cat2"
df_train['category_day']=df_train['day'].apply(category_day)
df_test['category_day']=df_test['day'].apply(category_day)

encoded_array = encoder.fit_transform(df_train[['category_day']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category_day']))

df_train = pd.concat([df_train.drop(columns=['category_day']), encoded_df], axis=1)

encoded_array = encoder.transform(df_test[['category_day']])

encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['category_day']))

df_test = pd.concat([df_test.drop(columns=['category_day']), encoded_df], axis=1)

df_test['category_day']=df_test['category_day_cat2']
df_test.drop(columns=['category_day_cat2'],inplace=True)

df_train['category_day']=df_train['category_day_cat2']
df_train.drop(columns=['category_day_cat2'],inplace=True)

df_train.category_day.value_counts()

df_fraud = df_train[df_train['is_fraud'] == 1]  # Keep only fraud transactions

ax = sns.histplot(data=df_fraud, x="day", common_norm=False, stat='percent', multiple='dodge')

ax.set_xticklabels(["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"])
ax.set_ylabel('Percentage')
ax.set_xlabel('Day of Week')
plt.title('Fraud Transactions by Day of the Week')
plt.show()

df_train['month']=pd.to_datetime(df_train['trans_date_trans_time']).dt.month
df_test['month']=pd.to_datetime(df_test['trans_date_trans_time']).dt.month
ax=sns.histplot(data=df_train, x="month", hue="is_fraud", common_norm=False,stat='percent',multiple='dodge')
ax.set_ylabel('Percentage')
ax.set_xlabel('Month')
plt.xticks(np.arange(1,13,1))
ax.set_xticklabels(["Jan","Feb","Mar","Apr","May","Jun","Jul",'Aug','Sep','Oct','Nov','Dec'])
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

df_fraud = df_train[df_train['is_fraud'] == 1]  # Keep only fraud transactions

ax = sns.histplot(data=df_fraud, x="month", common_norm=False, stat='percent', multiple='dodge')

ax.set_ylabel('Percentage')
ax.set_xlabel('Month')

plt.xticks(np.arange(1, 13, 1), ["Jan", "Feb", "Mar", "Apr", "May", "Jun",
                                 "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"])

plt.title('Fraud Transactions by Month')
plt.show()

df_train.head()

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np

X_train = df_train.drop(columns=['trans_date_trans_time','day','hour','is_fraud','month']).copy()
y_train = df_train['is_fraud'].copy()

X_test = df_test.drop(columns=['trans_date_trans_time','day','hour','is_fraud','month']).copy()
y_test = df_test['is_fraud'].copy()

#Feature Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)

X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)

class FraudDataset(Dataset):
    def __init__(self, features, labels):
        self.features = features
        self.labels = labels
    def __len__(self):
        return len(self.features)
    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

train_dataset = FraudDataset(X_train_tensor, y_train_tensor)
test_dataset = FraudDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=512, pin_memory=True)

class BinaryClassifier(nn.Module):
    def __init__(self, input_dim):
        super(BinaryClassifier, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(32, 1)
        )
    def forward(self, x):
        return self.model(x)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BinaryClassifier(X_train_tensor.shape[1]).to(device)

criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
# Training Loop
epochs = 20
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for features, labels in train_loader:
        features, labels = features.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(features)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    avg_loss = total_loss / len(train_loader)
    print(f"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}")

#  Evaluation
model.eval()
correct = 0
total = 0
with torch.no_grad():
    for features, labels in test_loader:
        features, labels = features.to(device), labels.to(device)
        outputs = model(features)
        predicted = torch.sigmoid(outputs)
        predicted = (predicted > 0.5).float()
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
accuracy = correct / total
print(f"Test Accuracy: {accuracy:.4f}")

from sklearn.metrics import precision_score, recall_score, f1_score

model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for features, labels in test_loader:
        features = features.to(device)
        outputs = model(features)
        preds = torch.sigmoid(outputs).cpu().numpy()
        all_preds.extend(preds)
        all_labels.extend(labels.numpy())

# Convert predictions to binary labels
# Ensure all_preds is a flat list or array before converting to binary
all_preds_flat = [p for sublist in all_preds for p in (sublist if isinstance(sublist, (list, np.ndarray)) else [sublist])]
binary_preds = [1 if p > 0.5 else 0 for p in all_preds_flat]

# Ensure all_labels is a flat list or array
all_labels_flat = [l for sublist in all_labels for l in (sublist if isinstance(sublist, (list, np.ndarray)) else [sublist])]

# Calculate metrics
precision = precision_score(all_labels_flat, binary_preds)
recall = recall_score(all_labels_flat, binary_preds)
f1 = f1_score(all_labels_flat, binary_preds)

print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")
print(f"F1 Score:  {f1:.4f}")

amt_saved = df_test[(binary_preds==y_test) & (df_test.is_fraud==1)].amt.sum()

amt_saved

fraud_amt = df_test[df_test.is_fraud==1].amt.sum()

fraud_amt

saved_percentage=amt_saved/fraud_amt*100

print(f'Using this model saves {saved_percentage} % of fraud money')

#dly

import gradio as gr
import torch
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import pandas as pd
import joblib

# Load the model components
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Assuming your model and scaler are saved like this after training
# torch.save(model.state_dict(), "fraud_model.pth")
# joblib.dump(scaler, "scaler.pkl")
# joblib.dump(encoder, "encoder.pkl")

torch.save(model.state_dict(), "fraud_model.pth")
joblib.dump(scaler, "scaler.pkl")
joblib.dump(encoder, "encoder.pkl")

class BinaryClassifier(torch.nn.Module):
    def __init__(self, input_dim):
        super(BinaryClassifier, self).__init__()
        self.model = torch.nn.Sequential(
            torch.nn.Linear(input_dim, 128),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(128, 64),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(64, 32),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(32, 1)
        )
    def forward(self, x):
        return self.model(x)

# Load scaler and encoders
scaler = joblib.load("scaler.pkl")
encoder = joblib.load("encoder.pkl")

# Instantiate and load model
input_dim = scaler.transform(np.zeros((1, scaler.mean_.shape[0]))).shape[1]
model = BinaryClassifier(input_dim)
model.load_state_dict(torch.load("fraud_model.pth", map_location=device))
model.to(device)
model.eval()

# Define prediction function
def predict_fraud(category, gender, amt, hour, day, month):
    # Preprocessing
    gender = 1 if gender == "M" else 0
    is_amt_greater_than_200 = 1 if amt >= 200 else 0
    time_category = "night" if (hour >= 22 or hour <= 3) else "day"
    category_day = "cat1" if (day == 0 or day >= 5) else "cat2"

    # One-hot encode category features
    category_encoded = encoder.transform([[category]]).flatten()
    time_cat_encoded = encoder.transform([[time_category]]).flatten()
    cat_day_encoded = encoder.transform([[category_day]]).flatten()

    # Prepare full feature vector
    input_features = np.concatenate([
        [amt, gender, is_amt_greater_than_200, hour, day, month],
        category_encoded,
        time_cat_encoded,
        cat_day_encoded
    ])

    # Feature scaling
    input_scaled = scaler.transform([input_features])
    input_tensor = torch.tensor(input_scaled, dtype=torch.float32).to(device)

    with torch.no_grad():
        output = torch.sigmoid(model(input_tensor))
        prediction = output.item()

    return {"Fraud Probability": round(prediction, 4), "Prediction": "Fraud" if prediction > 0.5 else "Not Fraud"}

# Gradio UI
category_options = ['grocery_pos', 'shopping_net', 'misc_net', 'shopping_pos', 'gas_transport', 'others']
interface = gr.Interface(
    fn=predict_fraud,
    inputs=[
        gr.Dropdown(category_options, label="Category"),
        gr.Radio(["M", "F"], label="Gender"),
        gr.Number(label="Transaction Amount (USD)"),
        gr.Slider(0, 23, step=1, label="Hour of Transaction"),
        gr.Slider(0, 6, step=1, label="Day of Week (0=Mon, 6=Sun)"),
        gr.Slider(1, 12, step=1, label="Month")
    ],
    outputs=[
        gr.Number(label="Fraud Probability"),
        gr.Text(label="Prediction")
    ],
    title="Fraud Detection Model",
    description="Enter transaction details to predict if it's fraudulent."
)

interface.launch()

df_fraud = df_train[df_train['is_fraud'] == 1]  # Keep only fraud transactions

ax = sns.histplot(data=df_fraud, x="month", common_norm=False, stat='percent', multiple='dodge')

ax.set_ylabel('Percentage')
ax.set_xlabel('Month')

plt.xticks(np.arange(1, 13, 1), ["Jan", "Feb", "Mar", "Apr", "May", "Jun",
                                 "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"])

plt.title('Fraud Transactions by Month')
plt.show()

df_train.head()

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np

X_train = df_train.drop(columns=['trans_date_trans_time','day','hour','is_fraud','month']).copy()
y_train = df_train['is_fraud'].copy()

X_test = df_test.drop(columns=['trans_date_trans_time','day','hour','is_fraud','month']).copy()
y_test = df_test['is_fraud'].copy()

#Feature Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)

X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)

class FraudDataset(Dataset):
    def __init__(self, features, labels):
        self.features = features
        self.labels = labels
    def __len__(self):
        return len(self.features)
    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

train_dataset = FraudDataset(X_train_tensor, y_train_tensor)
test_dataset = FraudDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=512, pin_memory=True)

class BinaryClassifier(nn.Module):
    def __init__(self, input_dim):
        super(BinaryClassifier, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(32, 1)
        )
    def forward(self, x):
        return self.model(x)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BinaryClassifier(X_train_tensor.shape[1]).to(device)

criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
# Training Loop
epochs = 20
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for features, labels in train_loader:
        features, labels = features.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(features)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    avg_loss = total_loss / len(train_loader)
    print(f"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}")

#  Evaluation
model.eval()
correct = 0
total = 0
with torch.no_grad():
    for features, labels in test_loader:
        features, labels = features.to(device), labels.to(device)
        outputs = model(features)
        predicted = torch.sigmoid(outputs)
        predicted = (predicted > 0.5).float()
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
accuracy = correct / total
print(f"Test Accuracy: {accuracy:.4f}")

from sklearn.metrics import precision_score, recall_score, f1_score

model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for features, labels in test_loader:
        features = features.to(device)
        outputs = model(features)
        preds = torch.sigmoid(outputs).cpu().numpy()
        all_preds.extend(preds)
        all_labels.extend(labels.numpy())

# Convert predictions to binary labels
# Ensure all_preds is a flat list or array before converting to binary
all_preds_flat = [p for sublist in all_preds for p in (sublist if isinstance(sublist, (list, np.ndarray)) else [sublist])]
binary_preds = [1 if p > 0.5 else 0 for p in all_preds_flat]

# Ensure all_labels is a flat list or array
all_labels_flat = [l for sublist in all_labels for l in (sublist if isinstance(sublist, (list, np.ndarray)) else [sublist])]

# Calculate metrics
precision = precision_score(all_labels_flat, binary_preds)
recall = recall_score(all_labels_flat, binary_preds)
f1 = f1_score(all_labels_flat, binary_preds)

print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")
print(f"F1 Score:  {f1:.4f}")

amt_saved = df_test[(binary_preds==y_test) & (df_test.is_fraud==1)].amt.sum()

amt_saved

fraud_amt = df_test[df_test.is_fraud==1].amt.sum()

fraud_amt

saved_percentage=amt_saved/fraud_amt*100

print(f'Using this model saves {saved_percentage} % of fraud money')

#dly

import gradio as gr
import torch
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import pandas as pd
import joblib

# Load the model components
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Assuming your model and scaler are saved like this after training
# torch.save(model.state_dict(), "fraud_model.pth")
# joblib.dump(scaler, "scaler.pkl")
# joblib.dump(encoder, "encoder.pkl")

torch.save(model.state_dict(), "fraud_model.pth")
joblib.dump(scaler, "scaler.pkl")
joblib.dump(encoder, "encoder.pkl")

class BinaryClassifier(torch.nn.Module):
    def __init__(self, input_dim):
        super(BinaryClassifier, self).__init__()
        self.model = torch.nn.Sequential(
            torch.nn.Linear(input_dim, 128),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(128, 64),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(64, 32),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3),
            torch.nn.Linear(32, 1)
        )
    def forward(self, x):
        return self.model(x)

# Load scaler and encoders
scaler = joblib.load("scaler.pkl")
encoder = joblib.load("encoder.pkl")

# Instantiate and load model
input_dim = scaler.transform(np.zeros((1, scaler.mean_.shape[0]))).shape[1]
model = BinaryClassifier(input_dim)
model.load_state_dict(torch.load("fraud_model.pth", map_location=device))
model.to(device)
model.eval()

# Define prediction function
def predict_fraud(category, gender, amt, hour, day, month):
    # Preprocessing
    gender = 1 if gender == "M" else 0
    is_amt_greater_than_200 = 1 if amt >= 200 else 0
    time_category = "night" if (hour >= 22 or hour <= 3) else "day"
    category_day = "cat1" if (day == 0 or day >= 5) else "cat2"

    # One-hot encode category features
    category_encoded = encoder.transform([[category]]).flatten()
    time_cat_encoded = encoder.transform([[time_category]]).flatten()
    cat_day_encoded = encoder.transform([[category_day]]).flatten()

    # Prepare full feature vector
    input_features = np.concatenate([
        [amt, gender, is_amt_greater_than_200, hour, day, month],
        category_encoded,
        time_cat_encoded,
        cat_day_encoded
    ])

    # Feature scaling
    input_scaled = scaler.transform([input_features])
    input_tensor = torch.tensor(input_scaled, dtype=torch.float32).to(device)

    with torch.no_grad():
        output = torch.sigmoid(model(input_tensor))
        prediction = output.item()

    return {"Fraud Probability": round(prediction, 4), "Prediction": "Fraud" if prediction > 0.5 else "Not Fraud"}

# Gradio UI
category_options = ['grocery_pos', 'shopping_net', 'misc_net', 'shopping_pos', 'gas_transport', 'others']
interface = gr.Interface(
    fn=predict_fraud,
    inputs=[
        gr.Dropdown(category_options, label="Category"),
        gr.Radio(["M", "F"], label="Gender"),
        gr.Number(label="Transaction Amount (USD)"),
        gr.Slider(0, 23, step=1, label="Hour of Transaction"),
        gr.Slider(0, 6, step=1, label="Day of Week (0=Mon, 6=Sun)"),
        gr.Slider(1, 12, step=1, label="Month")
    ],
    outputs=[
        gr.Number(label="Fraud Probability"),
        gr.Text(label="Prediction")
    ],
    title="Fraud Detection Model",
    description="Enter transaction details to predict if it's fraudulent."
)

interface.launch()
    return self.features[idx], self.labels[idx]

train_dataset = FraudDataset(X_train_tensor, y_train_tensor)
test_dataset = FraudDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=512, pin_memory=True)

class BinaryClassifier(nn.Module):
  def __init__(self, input_dim):
    super(BinaryClassifier, self).__init__()
    self.model = nn.Sequential(
      nn.Linear(input_dim, 128),
      nn.ReLU(),
      nn.Dropout(0.3),
      nn.Linear(128, 64),
      nn.ReLU(),
      nn.Dropout(0.3),
      nn.Linear(64, 32),
      nn.ReLU(),
      nn.Dropout(0.3),
      nn.Linear(32, 1)
    )
  def forward(self, x):
    return self.model(x)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BinaryClassifier(X_train_tensor.shape[1]).to(device)

criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
# Training Loop
epochs = 20
for epoch in range(epochs):
  model.train()
  total_loss = 0
  for features, labels in train_loader:
    features, labels = features.to(device), labels.to(device)
    optimizer.zero_grad()
    outputs = model(features)
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()
    total_loss += loss.item()
  avg_loss = total_loss / len(train_loader)
  print(f"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}")

# Evaluation
model.eval()
correct = 0
total = 0
with torch.no_grad():
  for features, labels in test_loader:
    features, labels = features.to(device), labels.to(device)
    outputs = model(features)
    predicted = torch.sigmoid(outputs)
    predicted = (predicted > 0.5).float()
    total += labels.size(0)
    correct += (predicted == labels).sum().item()
accuracy = correct / total
print(f"Test Accuracy: {accuracy:.4f}")

from sklearn.metrics import precision_score, recall_score, f1_score

model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
  for features, labels in test_loader:
    features = features.to(device)
    outputs = model(features)
    preds = torch.sigmoid(outputs).cpu().numpy()
    all_preds.extend(preds)
    all_labels.extend(labels.numpy())

# Convert predictions to binary labels
# Ensure all_preds is a flat list or array before converting to binary
all_preds_flat = [p for sublist in all_preds for p in (sublist if isinstance(sublist, (list, np.ndarray)) else [sublist])]
binary_preds = [1 if p > 0.5 else 0 for p in all_preds_flat]

# Ensure all_labels is a flat list or array
all_labels_flat = [l for sublist in all_labels for l in (sublist if isinstance(sublist, (list, np.ndarray)) else [sublist])]

# Calculate metrics
precision = precision_score(all_labels_flat, binary_preds)
recall = recall_score(all_labels_flat, binary_preds)
f1 = f1_score(all_labels_flat, binary_preds)

print(f"Precision: {precision:.4f}")
print(f"Recall:  {recall:.4f}")
print(f"F1 Score: {f1:.4f}")

amt_saved = df_test[(binary_preds==y_test) & (df_test.is_fraud==1)].amt.sum()

amt_saved

fraud_amt = df_test[df_test.is_fraud==1].amt.sum()

fraud_amt

saved_percentage=amt_saved/fraud_amt*100

print(f'Using this model saves {saved_percentage} % of fraud money')

#dly

import gradio as gr
import torch
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import pandas as pd
import joblib

# Load the model components
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Assuming your model and scaler are saved like this after training
# torch.save(model.state_dict(), "fraud_model.pth")
# joblib.dump(scaler, "scaler.pkl")
# joblib.dump(encoder, "encoder.pkl")

torch.save(model.state_dict(), "fraud_model.pth")
joblib.dump(scaler, "scaler.pkl")
joblib.dump(encoder, "encoder.pkl")

class BinaryClassifier(torch.nn.Module):
  def __init__(self, input_dim):
    super(BinaryClassifier, self).__init__()
    self.model = torch.nn.Sequential(
      torch.nn.Linear(input_dim, 128),
      torch.nn.ReLU(),
      torch.nn.Dropout(0.3),
      torch.nn.Linear(128, 64),
      torch.nn.ReLU(),
      torch.nn.Dropout(0.3),
      torch.nn.Linear(64, 32),
      torch.nn.ReLU(),
      torch.nn.Dropout(0.3),
      torch.nn.Linear(32, 1)
    )
  def forward(self, x):
    return self.model(x)

# Load scaler and encoders
scaler = joblib.load("scaler.pkl")
encoder = joblib.load("encoder.pkl")

# Instantiate and load model
input_dim = scaler.transform(np.zeros((1, scaler.mean_.shape[0]))).shape[1]
model = BinaryClassifier(input_dim)
model.load_state_dict(torch.load("fraud_model.pth", map_location=device))
model.to(device)
model.eval()

# Define prediction function
def predict_fraud(category, gender, amt, hour, day, month):
  # Preprocessing
  gender = 1 if gender == "M" else 0
  is_amt_greater_than_200 = 1 if amt >= 200 else 0
  time_category = "night" if (hour >= 22 or hour <= 3) else "day"
  category_day = "cat1" if (day == 0 or day >= 5) else "cat2"

  # One-hot encode category features
  category_encoded = encoder.transform([[category]]).flatten()
  time_cat_encoded = encoder.transform([[time_category]]).flatten()
  cat_day_encoded = encoder.transform([[category_day]]).flatten()

  # Prepare full feature vector
  input_features = np.concatenate([
    [amt, gender, is_amt_greater_than_200, hour, day, month],
    category_encoded,
    time_cat_encoded,
    cat_day_encoded
  ])

  # Feature scaling
  input_scaled = scaler.transform([input_features])
  input_tensor = torch.tensor(input_scaled, dtype=torch.float32).to(device)

  with torch.no_grad():
    output = torch.sigmoid(model(input_tensor))
    prediction = output.item()

  return {"Fraud Probability": round(prediction, 4), "Prediction": "Fraud" if prediction > 0.5 else "Not Fraud"}

# Gradio UI
category_options = ['grocery_pos', 'shopping_net', 'misc_net', 'shopping_pos', 'gas_transport', 'others']
interface = gr.Interface(
  fn=predict_fraud,
  inputs=[
    gr.Dropdown(category_options, label="Category"),
    gr.Radio(["M", "F"], label="Gender"),
    gr.Number(label="Transaction Amount (USD)"),
    gr.Slider(0, 23, step=1, label="Hour of Transaction"),
    gr.Slider(0, 6, step=1, label="Day of Week (0=Mon, 6=Sun)"),
    gr.Slider(1, 12, step=1, label="Month")
  ],
  outputs=[
    gr.Number(label="Fraud Probability"),
    gr.Text(label="Prediction")
  ],
  title="Fraud Detection Model",
  description="Enter transaction details to predict if it's fraudulent."
)

interface.launch()